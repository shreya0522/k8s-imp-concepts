1. What is Kubernetes and how does it work?
Kubernetes is a tool that helps you run and manage many containers across multiple servers. It handles scaling, deployment, and restarting containers automatically.

2. What is the difference between a Pod, ReplicaSet, Deployment, and StatefulSet?
Pod: Smallest unit in Kubernetes. It runs one or more containers together.
ReplicaSet: Ensures a set number of Pods are always running.
Deployment: Manages ReplicaSets and updates Pods easily.
StatefulSet: Like Deployment but used for apps that need stable names or storage (like databases).

3. Why are Pods ephemeral?
Pods are temporary‚Äîthey can be deleted or replaced at any time due to scaling or failures. That‚Äôs why they are not meant to store permanent data.

4. What is the role of the Kubelet?
Kubelet runs on each node. It makes sure the containers in the Pods are running and healthy.

5. Explain the difference between kubectl apply and kubectl create.
kubectl create: Used to create a resource for the first time.
kubectl apply: Used to create or update a resource. Good for making changes.

6. What is a Namespace and why is it used?
A Namespace is a way to divide cluster resources between users or teams. It helps organize and manage large clusters.

# ===============================================================================================================================================

üèóÔ∏è 2. Architecture & Components

üß± 1. Describe the Kubernetes architecture.
Kubernetes has a Control Plane and Worker Nodes. The Control Plane manages the cluster.
Worker Nodes run the actual applications in Pods.
Key Components:
* Control Plane: API Server, Scheduler, Controller Manager, etcd
* Node Components: Kubelet, Kube Proxy, Container Runtime
Follow-up Interview Point:
üëâ ‚ÄúHow do these components talk to each other?‚Äù
Answer: Mostly through the Kube API Server, which acts as the central communication point.

üß© 2. What are the components of the Control Plane?
API Server ‚Äì Frontend for the cluster, all requests go through this.
Scheduler ‚Äì Places new Pods on suitable Nodes.
Controller Manager ‚Äì Handles background tasks (like scaling, restarts). amke sure cluster stays in desired state
etcd ‚Äì Stores the cluster‚Äôs configuration and state (key-value store).

Follow-up Interview Point:
üëâ ‚ÄúIs API Server stateless?‚Äù
Yes. API Server is stateless and can be horizontally scaled.

Behind-the-scenes:
Controllers watch the cluster state via API Server, then make changes by writing back to it. etcd stores all this information.

üì¶ 3. How does the Scheduler decide where to place a pod?
Simple Answer:
The Scheduler looks at available nodes and decides based on:
- Resource availability (CPU, RAM)
- Node labels, taints/tolerations
- Affinity/anti-affinity rules

Follow-up Interview Point:
üëâ ‚ÄúCan we influence the scheduler decision?‚Äù
Yes, using nodeSelector, affinity rules, and taints/tolerations.

Behind-the-scenes:
Scheduler gets a list of candidate nodes from the API Server, applies filters, then scores them, and finally selects the best match.

üß† 4. What is the role of etcd?
etcd is a distributed key-value store that stores all cluster data‚Äîlike config, status, secrets, etc.

Follow-up Interview Point:
üëâ ‚ÄúWhat happens if etcd fails?‚Äù
Cluster can‚Äôt function fully‚Äîno scheduling or state management.
Important: Take etcd backups regularly‚Äîlosing etcd = losing cluster state.

Behind-the-scenes: All changes (like pod creation, scaling) are written to etcd via API Server, and controllers act based on this data.

‚öôÔ∏è 5. Explain the Controller Manager and what controllers it runs.
Controller Manager runs various controllers that make sure the cluster stays in the desired state.
Common Controllers:
- ReplicationController / ReplicaSet
- Node Controller (monitors node health)
- Job Controller
- Endpoint Controller

Follow-up Interview Point:
üëâ ‚ÄúWhat happens when a node goes down?‚Äù
Node Controller notices and marks it as NotReady, and other controllers may restart pods elsewhere.

Behind-the-scenes:
Each controller watches etcd (via API Server), compares current vs. desired state, and makes changes.

üì¶ 6. What is the Container Runtime Interface (CRI)?
The CRI (Container Runtime Interface) is a standard API. It defines how Kubernetes talks to container runtimes like containerd, CRI-O, etc. This means Kubernetes doesn‚Äôt care who makes the runtime ‚Äî as long as it follows the CRI standard, Kubernetes can talk to it.
Kubernetes talks to the container runtime (like containerd) through CRI. This standardizes how Kubernetes tells the runtime to:
- Start containers
- Stop containers
- Pull images
- Check container status

Follow-up Interview Point:
üëâ ‚ÄúIs Docker a runtime now?‚Äù
- Docker support was deprecated in Kubernetes 1.20+. Use containerd now.
What happened to Docker in Kubernetes?
Kubernetes deprecated Docker as a runtime starting from v1.20. It doesn‚Äôt mean Docker images won‚Äôt run ‚Äî you can still build images using Docker, but Kubernetes uses containerd under the hood to run them.

- Important Point: CRI ensures flexibility‚Äîyou can plug in different runtimes without changing Kubernetes core. The Kubelet uses CRI to start/stop containers on a node.

Behind-the-scenes:
Kubelet calls the CRI endpoint (like containerd's API) to manage containers in Pods.

7. How does the Kubelet use CRI to start/stop containers?
       > What is the Kubelet?
          The **Kubelet** is the agent that runs on every **worker node** in Kubernetes. Its job is to:
                           * Talk to the Kubernetes API server
                           * Ensure the containers (pods) defined in the API actually run on the node
                           * Report the node's status back to the control plane

      >  What is the CRI?
         CRI = Container Runtime Interface.It‚Äôs a **standard API** that the Kubelet uses to **talk to the container runtime** on the node ‚Äî such as `containerd` or `CRI-O`.

      >  So, how does the Kubelet use CRI to start/stop containers?
             Here‚Äôs what happens **step by step** when the Kubelet gets instructions from the control plane:
              Example: Kubelet starting a Pod
                       1. **Kubernetes API Server** sends a pod spec (YAML) to the Kubelet.
                       2. The **Kubelet parses the pod spec**.
                       3. The Kubelet makes **gRPC API calls** (defined by the CRI) to the container runtime (like containerd), asking it to:
                             * Pull the required container image (`PullImage`)
                             * Create a sandbox for the pod (`RunPodSandbox`)
                             * Start containers (`CreateContainer`, `StartContainer`)
                       4. The container runtime **executes those instructions** and runs the containers.
            Example: Stopping Containers
                      If the pod is deleted or the container crashes, the Kubelet uses CRI calls like:
                             * StopContainer
                             * RemoveContainer
                             * RemovePodSandbox

          Diagram (Simplified):

                                  Control Plane
                                       ‚Üì
                                  API Server
                                       ‚Üì
                             Kubelet (on Worker Node)
                                       ‚Üì 
                            uses CRI (standard gRPC API)
                                       ‚Üì 
                          Container Runtime (containerd / CRI-O)
                                       ‚Üì
                      Runs the actual containers on the node

# =========================================================================================================================================================
 ## üß† Kubelet ‚Äì Node Agent
------------------------------------------------------------------------------------------------------------------------------------------------------------
1-How does the Kubelet detect changes in PodSpecs from the API server?
2-What is a Pod Sandbox, and how does the Kubelet manage it?
3-How does the Kubelet decide when to restart a failed container?
4-What happens if the container runtime is down, but the Kubelet is running?
5-How does the Kubelet determine resource availability for scheduling pods?

üß† 1. How does the Kubelet detect changes in PodSpecs from the API server?
Answer: Kubelet uses the Kubernetes API Server watch mechanism to monitor Pod changes.

Workflow:
* Kubelet registers itself with the API Server.
* It watches for PodSpecs (bound to its node) via /api/v1/pods.
* When a new or updated PodSpec is detected, Kubelet:
       - Validates it
       - Pulls container images
       - Starts containers via the Container Runtime Interface (CRI)

Follow-up:
üëâ ‚ÄúDoes Kubelet poll the API server?‚Äù
No, it uses watch (via long-lived HTTP connection) for real-time updates.

-------------------------------------------------------------------------------------------------

üß± 2. What is a Pod Sandbox, and how does the Kubelet manage it?
Answer: A Pod Sandbox is a special container that sets up the network namespace and security context for all containers in a Pod.

Workflow:
* Kubelet calls the CRI to create a Pod Sandbox.
* It sets up:
      -Network (IP address, DNS, hostname)
      -cgroups and namespaces
      -Then Kubelet launches containers inside that sandbox.

Key Point:
If the Pod Sandbox fails, all containers in that Pod are recreated.

Follow-up:
üëâ ‚ÄúHow do you check if the Pod sandbox is healthy?‚Äù
Check kubectl describe pod ‚Üí under Events ‚Üí look for FailedSandbox.

-------------------------------------------------------------------------------------------------

3. How does the Kubelet decide when to restart a failed container?
Answer: It follows the Pod's restartPolicy (Always, OnFailure, or Never).

Workflow:
    - Kubelet watches container status via CRI (containerd).
    - On failure (non-zero exit code):
          *If restartPolicy: Always ‚Üí restart it immediately.
          *If OnFailure ‚Üí restart only on non-zero exit.
          *If Never ‚Üí don‚Äôt restart.

Important:
Restart logic is Kubelet‚Äôs job, not controlled by the container runtime.

Follow-up:
üëâ ‚ÄúWhere is the backoff delay handled?‚Äù
Kubelet applies exponential backoff to avoid rapid restarts.

üëâ how kubelet watches containers 
1-Kubelet communicates with the container runtime (e.g., containerd) through the CRI gRPC API.
2-It uses CRI methods like:
    * ListContainers()  *ContainerStatus()   *ListPodSandbox()
3- Kubelet does not directly "watch" container state like the API Server watch mechanism. Instead, it polls container runtime status regularly through CRI calls.
4- Based on container status (e.g., exit code, health), Kubelet takes actions like:
    *Restarting containers *Reporting to the API Server  *Updating Pod status

-------------------------------------------------------------------------------------------------

‚ö†Ô∏è 4. What happens if the container runtime is down, but the Kubelet is running?
Answer:Kubelet cannot manage containers‚ÄîPod creation, deletion, and health checks will fail.

Workflow:
    - Kubelet pings CRI socket (e.g., /run/containerd/containerd.sock)
    - If unreachable:
              * Kubelet logs errors
              * Containers can't be created/stopped
              * Node may go NotReady if health checks fail

    Follow-up:
üëâ ‚ÄúWill running containers keep working?‚Äù
Yes, existing containers may keep running, but Kubelet can‚Äôt monitor or restart them

-------------------------------------------------------------------------------------------------

üìä 5. How does the Kubelet determine resource availability for scheduling pods?
Answer: Kubelet doesn‚Äôt schedule pods‚Äîit reports node resource status to the API Server, and the Scheduler makes decisions.

Workflow:
* Kubelet uses cAdvisor to collect:
      - CPU, memory usage
      - Disk and network stats
* Sends these metrics in a NodeStatus update to API Server.
* Scheduler uses this info to decide where to place pods.

Important:
Kubelet also enforces resource limits defined in the Pod (requests and limits) via cgroups.

Follow-up:
üëâ ‚ÄúCan Kubelet reject a pod?‚Äù
Yes, if the Pod requests more resources than available, Kubelet may reject it (esp. with CPU/memory pressure).

# =========================================================================================================================================================
##   üåê kube-proxy ‚Äì Networking
-----------------------------------------------------------------------------------------------------------------------------------------------------------

#### what kube proxy is ? 
#### It's a networking component that runs on each node in a Kubernetes cluster.Its job is to make the Service IPs (like ClusterIP) actually work by sending traffic to the #### right backend Pods.
-
-----------------------------------------------------------------------------------------------------------------------------------------------------------
üåê 1. How does kube-proxy manage Service IPs and route traffic to pods?
Answer: 
What kube-proxy is:
    * It's a **networking component** that runs on **each node** in a Kubernetes cluster.
    * Its job is to **make the Service IPs (like ClusterIP) actually work** by sending traffic to the right backend Pods.

What happens when a Service is created?**
     1. You define a **Service** (e.g., `my-service`) of type `ClusterIP`.
     2. Kubernetes gives it a **virtual IP** (like `10.0.0.5`) inside the cluster.
     3. The Service **selects Pods** using a label selector (e.g., all Pods with label `app=web`).
     4. Kubernetes creates an **Endpoints object**, which lists the IP addresses and ports of the selected Pods (example: `10.244.1.7:80`, `10.244.2.3:80`).

What kube-proxy does:
         Now kube-proxy comes into action:
  Step 1: Watches for changes: kube-proxy is watching the Kubernetes API Server** for any changes in:
              * Service objects
              * Endpoints objects

  Step 2: Creates routing rules
         * kube-proxy reads the Service IP and backend Pod IPs.
         * Based on your OS and kube-proxy settings, it sets up either:
         * iptables rules (default in most clusters), or
         * IPVS rules (for better performance).

  These rules say: "If anyone tries to access 10.0.0.5:80, forward the request to one of these real Pods: 10.244.1.7:80 or 10.244.2.3:80."

  What happens during a real request:
           Let‚Äôs say a pod in the cluster does: ``` curl http://10.0.0.5:80```
             

 Behind the scenes:
          1. The **Linux networking stack** hits an **iptables rule** created by kube-proxy.
          2. This rule **redirects the request to one of the Pod IPs** randomly (load-balancing).
          3. The request **never even reaches kube-proxy as a process** ‚Äî Linux handles it directly using the rules kube-proxy set earlier.

 So kube-proxy **programs the rules**, but **doesn‚Äôt touch every packet**.

 How kube-proxy load-balances traffic:
          * In iptables mode: It uses round-robin to forward to different Pods.
           *In IPVS mode**: It supports advanced load balancing algorithms like:
                   * Round Robin   * Least Connections   * Source Hashing
             You can choose IPVS by passing: ``` --proxy-mode=ipvs ```


Bonus: What are ClusterIP and Endpoints?
        * ClusterIP: is just a virtual IP ‚Äî not tied to any real interface.  It only works inside the cluster.
        * The **Endpoints** object holds the **real Pod IPs** to which traffic should go.

---

 Summary (visualized):
```
User Pod (curl 10.0.0.5:80)
   ‚Üì
Linux Networking Stack
   ‚Üì
iptables rule (set by kube-proxy)
   ‚Üì
Real Pod IP (e.g., 10.244.1.7:80)
```

-----------------------------------------------------------------------------------------------------------------------------------------------------------

üîÅ 2. What is the difference between iptables and IPVS modes in kube-proxy?
Both are proxy modes that kube-proxy can use to manage how traffic is routed from a Service IP to the actual Pod IPs.

üîπ  iptables mode ‚Äì Traditional default
üí° How it works:
           kube-proxy creates iptables rules (Linux firewall rules). 
            Every time a request hits a Service IP (like 10.0.0.5), the kernel checks the iptables rules and redirects the packet to one of the backend Pods.
‚öôÔ∏è Workflow:
       Example rule: If traffic is for 10.0.0.5:80, redirect to one of: 10.244.1.7:80  10.244.2.5:80
       Load balancing is done using random or statistic match in iptables.

üìâ Drawbacks:
        * Performance degrades when there are many Services and Pods because:
        * iptables is rule-order dependent ‚Äî it has to process one rule at a time in order.
        * Every change (new Pod/Service) causes full rebuild of the rule chain ‚Äî time-consuming.

üîπ 2. IPVS mode ‚Äì Advanced and optimized
üí° How it works:
           Uses Linux IPVS (IP Virtual Server), a built-in kernel-level load balancer.
           kube-proxy programs IPVS rules using the ipvsadm tool or kernel APIs.
           The kernel then uses a hash table, not a rule chain, for routing ‚Äî much faster.

‚öôÔ∏è Workflow:   Maintains a service table in the kernel:
                ``` IPVS: 10.0.0.5:80 ‚Üí [10.244.1.7:80, 10.244.2.5:80] ```              
                 Picks one Pod using one of many algorithms: Round Robin ,Least Connections, Source IP Hash

üöÄ Advantages: Much faster routing, especially in large clusters (1000s of Services/Pods).
               Rule updates are incremental ‚Äî only changes are applied, not full table rewrite.


üîÅ Side-by-side comparison:
| Feature         | **iptables**                           | **IPVS**                                |
| --------------- | -------------------------------------- | --------------------------------------- |
| **Mode**        | Sequential rule matching (`netfilter`) | Kernel-based virtual server             |
| **Performance** | Slower with scale                      | Faster, especially in large clusters    |
| **Rule Mgmt**   | Full table rebuild on updates          | Only affected entries updated           |
| **Algorithms**  | Only basic round-robin/statistic       | Many (Round Robin, Least Conn, etc.)    |
| **Memory use**  | Lighter, good for small clusters       | Slightly higher but scalable            |
| **Stability**   | Stable, default in most clusters       | Stable but needs IPVS modules installed |

üß† Key Point:
          *  Use iptables in small/simple clusters.
          * Prefer IPVS in production or large-scale clusters for: Faster failovers , Scalability , Advanced load balancing

üõ†Ô∏è How to enable IPVS mode: 
         In your kube-proxy config or DaemonSet:
``` 
command:
  - kube-proxy
  - --proxy-mode=ipvs
```
Also ensure the node has IPVS kernel modules:
``` lsmod | grep ip_vs ```

------------------------------------------------------------------------------------------------------------------------------------------------------------

üîÑ 3. How does kube-proxy handle updates to Service or Endpoints objects?
Answer: kube-proxy watches the Kubernetes API Server for changes to Service and Endpoints objects, and updates its routing rules (iptables or IPVS) accordingly ‚Äî all this happens automatically and with minimal traffic disruption.

üß† What triggers an update?
When something changes like: A new Pod is created or deleted,  A Service selector is updated, A Pod becomes Ready/Unready (so its IP should be added/removed).
This causes the Endpoints object to change, and that triggers kube-proxy to act.

‚öôÔ∏è Behind the scenes: Workflow
1- üïµÔ∏è‚Äç‚ôÇÔ∏è kube-proxy is continuously watching the API Server (using a Watch API). It monitors Service and Endpoints objects.
2- üîÑ If an update is detected:  For example: a Pod is deleted ‚Üí the Endpoints list is now shorter.
3- üîç kube-proxy compares the old state and the new state of these objects.
    - What was added?  -What was removed? -What changed?
4- üõ†Ô∏è Based on the difference, it updates the networking rules:
        - If using iptables: it rebuilds the relevant rules.
        - If using IPVS: it incrementally updates the internal routing tables.
5 üöÄ These changes take effect immediately, allowing traffic to be routed to the new set of healthy Pod IPs.

üì¶ Summary:
| Step            | What Happens                               |
| --------------- | ------------------------------------------ |
| Watch           | kube-proxy watches API Server for changes  |
| Detect          | Notices change in Service or Endpoints     |
| Compare         | Old vs new configuration                   |
| Update Rules    | Updates iptables/IPVS with correct Pod IPs |
| Forward Traffic | Keeps traffic flowing to valid Pods only   |

Follow-up:
üëâ ‚ÄúIs the traffic affected during update?‚Äù
Minimal or no downtime if kube-proxy is healthy.

------------------------------------------------------------------------------------------------------------------------------------------------------------

üîÑ 4. Can kube-proxy be replaced or customized? (e.g., Cilium)
Answer: Yes, kube-proxy can be replaced by eBPF-based networking solutions like Cilium.
Common Alternatives:
> Cilium: Uses eBPF, replaces kube-proxy for high-performance routing.
> Calico, Kube-router: May also replace kube-proxy functionality.

Why replace it?
 - Better performance
 - Better observability
 - Native support for Network Policies

Follow-up:
üëâ ‚ÄúWhat is eBPF?‚Äù
Extended Berkeley Packet Filter ‚Äì allows fast packet processing in the Linux kernel without modifying kernel code.

------------------------------------------------------------------------------------------------------------------------------------------------------------

üßπ 5. What happens in kube-proxy if a pod behind a service is deleted?
Answer:  When a Pod is deleted, the corresponding Endpoints object for the Service is updated, and kube-proxy removes that Pod‚Äôs IP from its routing rules ‚Äî so traffic is no longer routed to the deleted Pod.

‚öôÔ∏è Workflow (Step-by-step):
  1- ‚ùå A Pod (say 10.244.2.5) is deleted (manually or due to a crash).
  2- üß† The Kubernetes Endpoint controller detects this change. It updates the Endpoints object of the Service to remove 10.244.2.5.
  3- üïµÔ∏è‚Äç‚ôÇÔ∏è kube-proxy is watching for these updates via the API Server (watch)
  4- üîÑ When it notices the Endpoints list has changed: > It updates iptables or IPVS rules. > Removes the rule forwarding to the deleted Pod.
  5- üö´ Now, traffic going to the Service IP will not be forwarded to the dead Pod.
  6- ‚úîÔ∏è Users/Clients won‚Äôt hit the deleted Pod anymore ‚Äî it‚Äôs cleanly removed from routing.

üîç Behind the scenes (technical flow):
     * The Pod‚Äôs IP is removed from: Endpoints object of the Service (handled by Kube controller manager).
     * This triggers an event, which kube-proxy is subscribed to via a watch.
     * Based on your proxy mode:
          - iptables: the rules redirecting traffic to the Pod IP are removed.
          - IPVS: the backend server entry is deleted from the kernel‚Äôs load balancer.

‚ùì Follow-up Q: "Is there a delay in removal? 
Answer: Usually the delay is a few seconds or less ‚Äî kube-proxy is fast.
But delays can happen if:
      - The API Server is under load.
      - kube-proxy is slow to process events.
      - The Pod termination signal is delayed (e.g., in a crash or node failure).
To reduce risk of stale traffic:
      - Use readiness/liveness probes to mark Pods "unready" before deletion.
      - Consider using graceful termination periods in deployments.

------------------------------------------------------------------------------------------------------------------------------------------------------------

# =========================================================================================================================================================
##   üì¶ API Server ‚Äì Cluster Gateway 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
1- üì° How does the API server ensure high availability and leader election? How does Kubernetes make sure the API server stays up and available even if one instance goes down?
The API Server is made highly available by running multiple replicas behind a load balancer. The leader election is relevant for certain control plane components (like controller-manager or scheduler), not the API server itself ‚Äî since all API server replicas are active, not just one.

üß± High Availability (HA) of API Server
‚öôÔ∏è Workflow:
1- You deploy multiple API server pods or instances (e.g., 3).
2- These are placed behind a load balancer (like AWS ELB or HAProxy).
3- Clients (like kubectl, kubelet, controllers) send requests to the load balancer.
4- The load balancer distributes traffic among healthy API servers.
5- All replicas are stateless ‚Äî they store state in etcd, not locally.

üìå Important: API servers do not need leader election because they serve requests in parallel.

ü§ù Leader Election ‚Äì Applies to Controller Manager and Scheduler
For components like kube-controller-manager and kube-scheduler, only one instance should be active at a time, even if multiple replicas are running.
üí° How it works:
- All controller-manager/scheduler replicas try to become leader.
- They attempt to acquire a lease lock (usually stored in a Kubernetes ConfigMap or Lease object).
- One becomes the leader; others stay in standby mode.
- If the leader goes down: Others detect the lost lease.A new leader is elected automatically.
This is handled using Kubernetes leader-election library (based on distributed coordination via etcd).

üîê Behind the Scenes (Leader Election):
- Lease object stored in Kubernetes API (e.g., /kube-system/leader-election).
- Uses etcd as the backend data store.
- Election happens by writing to the lease object:
      * First one to claim the lease wins.
      * Periodic renewal ensures leader is still alive.
      * Timeout ‚Üí others can take over.

     üß™ Example Use Case:
   Let‚Äôs say 2 replicas of kube-controller-manager are running: Replica 1 becomes leader (writes lease).It manages deployments, replicasets, etc.Replica 2 does nothing ‚Äî 
   just waits.If replica 1 dies, replica 2 sees lease timeout ‚Üí takes over.

‚ùì Follow-Up Questions You Might Be Asked:
Q: Why does the API server not require leader election?
A: Because all API server instances are stateless and can serve traffic concurrently.

Q: What happens if etcd goes down?
A: The API servers will not function fully ‚Äî they can‚Äôt store or retrieve cluster state.

Q: How is the health of API servers checked?
A: The load balancer uses health checks (e.g., /healthz) to route traffic only to healthy servers.

-----------------------------------------------------------------------------------------------------------------------------------------------------------
2 - ‚ùì What are Admission Controllers?
They are checkpoints inside the API server that act like gatekeepers for any create, update, delete requests in Kubernetes.

üì¶ Imagine this workflow:
      > You run:  ```kubectl create pod mypod.yaml```
      > Here‚Äôs what happens step-by-step:
           * ‚úÖ Authentication ‚Äì Are you a valid user?
           * ‚úÖ Authorization ‚Äì Are you allowed to create a Pod?
           * üõë Admission Controllers ‚Äì Should we allow or reject this Pod based on: Rules, Security, Custom policies, Auto-editing (e.g., adding labels)
üíæ If allowed ‚Üí object is saved into etcd (the cluster database).

üéØ Why Admission Controllers are important:
           > They can stop risky or invalid requests.
           > They can automatically add required config (like labels or sidecar containers).
           > They are the last safety check before a resource is accepted into the cluster.
üìå Example: You try to create a Pod running as root. If PodSecurity Admission Controller is enabled, it can deny that Pod creation based on policy.

üîç Common Admission Controllers:
| Controller Name              | What it does                               |
| ---------------------------- | ------------------------------------------ |
| `NamespaceLifecycle`         | Blocks resources in terminating namespaces |
| `LimitRanger`                | Enforces resource limits                   |
| `PodSecurity`                | Blocks insecure Pods                       |
| `MutatingAdmissionWebhook`   | Changes the object (e.g., injects sidecar) |
| `ValidatingAdmissionWebhook` | Validates but doesn't modify               |

-----------------------------------------------------------------------------------------------------------------------------------------------------------

3 üîê What is Authentication vs Authorization in the API Server?
‚úÖ Short Answer: Authentication: Confirms who you are. Authorization: Decides what you are allowed to do.

‚öôÔ∏è Workflow (Request Processing):
        > A request hits the API Server (e.g., kubectl get pods)
        > The API server checks:  Authentication ‚Üí Is this user/token valid? Authorization ‚Üí Is the user allowed to perform this action on this resource? 
         If both pass ‚Üí Admission Controllers ‚Üí Save to etcd

üîç Authentication Methods:
    - Certificates  - Bearer tokens  - OIDC (OAuth2)  - Static password/token files (for testing)

üîç Authorization Modes:
    - RBAC (Role-Based Access Control) ‚úÖ most common  - ABAC  - Node  - Webhook

‚ùì Follow-up Questions You Might Get:
Q: ‚ÄúWhat happens if authn passes but authz fails?‚Äù
A: The request is rejected with a 403 Forbidden.

Q: ‚ÄúCan multiple authorization modes be enabled?‚Äù
A: Yes ‚Äî the API server processes them in order and uses the first non-abstaining decision.

-----------------------------------------------------------------------------------------------------------------------------------------------------------

üö¶ How does the API server handle excessive request loads (rate limiting)?
‚úÖ Short Answer: The Kubernetes API Server protects itself from overload by using rate limiting, priority levels, and concurrency controls. This ensures fair usage 
                  and prevents any user or component from overwhelming the server.

‚öôÔ∏è Mechanisms Used:
  1. Client-side Rate Limiting (via kubelet, kubectl, etc. Clients like kubectl or kubelet have built-in limits: Example: default 5 QPS (queries 
                  per second) and 10 burst for kubelet. You can configure it: ``` kubectl --request-timeout=30s --v=9 ```

  2. API Server Priority and Fairness (APF): 
                    * Since Kubernetes v1.20+, the API server uses Priority and Fairness (APF).
                    * It divides traffic into ‚Äúpriority levels‚Äù and ‚Äúflows‚Äù, like: > System components (high priority)  > User requests (medium)  > Background jobs (low)
             üîÅ Each flow gets a fair share of API server resources. If a request exceeds its limit: It is either queued or rejected with 429 Too Many Requests.

üîê Example Flow:
           > You have a script sending 1000 pod creation requests.
           > API server groups those requests into a ‚Äúflow‚Äù.
           > If that flow is over its fair share, Kubernetes:
           > Queues some requests temporarily.
            > Drops or delays others to avoid overloading etcd and the API server.


‚ùì Follow-up Questions You May Get:
Q: What happens when the API server gets a lot of requests at once?
A: It queues or throttles requests based on priority/fairness.

Q: What status code is returned when throttled?
A: HTTP 429 Too Many Requests.

Q: How can you control a component‚Äôs rate?
A: By adjusting client flags like --kube-api-qps and --kube-api-burst.








