# =========================================================================================================================================================
# ğŸ—ï¸ 1. Basic
------------------------------------------------------------------------------------------------------------------------------------------------------------
1. What is Kubernetes and how does it work?

Kubernetes is a tool that helps you run and manage many containers across multiple servers. It handles scaling, deployment, and restarting containers automatically.

-----------------------------------------------------------------

2. What is the difference between a Pod, ReplicaSet, Deployment, and StatefulSet?

Pod: Smallest unit in Kubernetes. It runs one or more containers together.
ReplicaSet: Ensures a set number of Pods are always running.
Deployment: Manages ReplicaSets and updates Pods easily.
StatefulSet: Like Deployment but used for apps that need stable names or storage (like databases).

-----------------------------------------------------------------

3. Why are Pods ephemeral?

Pods are temporaryâ€”they can be deleted or replaced at any time due to scaling or failures. Thatâ€™s why they are not meant to store permanent data.

-----------------------------------------------------------------

4. What is the role of the Kubelet?

Kubelet runs on each node. It makes sure the containers in the Pods are running and healthy.

-----------------------------------------------------------------

5. Explain the difference between kubectl apply and kubectl create.

kubectl create: Used to create a resource for the first time.
kubectl apply: Used to create or update a resource. Good for making changes.

-----------------------------------------------------------------

6. What is a Namespace and why is it used?

A Namespace is a way to divide cluster resources between users or teams. It helps organize and manage large clusters.

-----------------------------------------------------------------

# =========================================================================================================================================================
# ğŸ—ï¸ 2. Architecture & Components
------------------------------------------------------------------------------------------------------------------------------------------------------------

ğŸ§± 1. Describe the Kubernetes architecture.

 Kubernetes has a Control Plane and Worker Nodes. The Control Plane manages the cluster.
 Worker Nodes run the actual applications in Pods.
Key Components:

* Control Plane: API Server, Scheduler, Controller Manager, etcd
* Node Components: Kubelet, Kube Proxy, Container Runtime

Follow-up Interview Point:
- â€œHow do these components talk to each other?â€
- Answer: Mostly through the Kube API Server, which acts as the central communication point.

-----------------------------------------------------------------

ğŸ§© 2. What are the components of the Control Plane?

(CASE)
- Controller Manager â€“ Handles background tasks (like scaling, restarts). make sure cluster stays in desired state
- API Server â€“ Frontend for the cluster, all requests go through this.
- Scheduler â€“ Places new Pods on suitable Nodes.
- Etcd â€“ Stores the clusterâ€™s configuration and state (key-value store).

Follow-up Interview Point:
--------------------------
ğŸ‘‰ â€œIs API Server stateless?â€
* Yes. API Server is stateless and can be horizontally scaled.

#### Behind-the-scenes:
> Controllers watch the cluster state via API Server, then make changes by writing back to it. ETCD stores all this information.

-----------------------------------------------------------------

ğŸ“¦ 3. How does the Scheduler decide where to place a pod?

Simple Answer:
- The Scheduler looks at available nodes and decides based on:
- Resource availability (CPU, RAM)
- Node labels, taints/tolerations
- Affinity/anti-affinity rules

Follow-up Interview Point:
- â€œCan we influence the scheduler decision?â€
- Yes, using nodeSelector, affinity rules, and taints/tolerations.

Behind-the-scenes:
- Scheduler gets a list of candidate nodes from the API Server, applies filters, then scores them, and finally selects the best match.

-----------------------------------------------------------------

4. What is the role of etcd?

ETCD is a distributed key-value store that stores all cluster data â€” like config, status, secrets, etc.

Follow-up Interview Point:
ğŸ‘‰ â€œWhat happens if etcd fails?â€
Cluster canâ€™t function fullyâ€”no scheduling or state management.
Important: Take etcd backups regularlyâ€”losing etcd = losing cluster state.

Behind-the-scenes: 
- All changes (like pod creation, scaling) are written to etcd via API Server, and controllers act based on this data.

-----------------------------------------------------------------

5. Explain the Controller Manager and what controllers it runs.

Controller Manager runs various controllers that make sure the cluster stays in the desired state.
> Common Controllers:
- Replication Controller / ReplicaSet
- Node Controller (monitors node health)
- Job Controller
- Endpoint Controller

Follow-up Interview Point:
ğŸ‘‰ â€œWhat happens when a node goes down?â€
Node Controller notices and marks it as NotReady, and other controllers may restart pods elsewhere.

Behind-the-scenes:
Each controller watches etcd (via API Server), compares current vs. desired state, and makes changes.

-----------------------------------------------------------------

ğŸ“¦ 6. What is the Container Runtime Interface (CRI)?

The CRI (Container Runtime Interface) is a standard API. It defines how Kubernetes talks to container runtimes like containerd, CRI-O, etc. This means Kubernetes doesnâ€™t care who makes the runtime â€” as long as it follows the CRI standard, Kubernetes can talk to it.
Kubernetes talks to the container runtime (like containerd) through CRI. This standardizes how Kubernetes tells the runtime to:
- Start containers
- Stop containers
- Pull images
- Check container status

Follow-up Interview Point:

ğŸ‘‰ â€œIs Docker a runtime now?â€

- Docker support was deprecated in Kubernetes 1.20+. Use containerd now.
What happened to Docker in Kubernetes?
Kubernetes deprecated Docker as a runtime starting from v1.20. It doesnâ€™t mean Docker images wonâ€™t run â€” you can still build images using Docker, but Kubernetes uses containerd under the hood to run them.

- Important Point: CRI ensures flexibilityâ€”you can plug in different runtimes without changing Kubernetes core. The Kubelet uses CRI to start/stop containers on a node.

Behind-the-scenes:
Kubelet calls the CRI endpoint (like containerd's API) to manage containers in Pods.

-----------------------------------------------------------------

7. How does the Kubelet use CRI to start/stop containers?

       > What is the Kubelet?

          The **Kubelet** is the agent that runs on every **worker node** in Kubernetes. Its job is to:
                           * Talk to the Kubernetes API server
                           * Ensure the containers (pods) defined in the API actually run on the node
                           * Report the node's status back to the control plane

      >  What is the CRI?
         CRI = Container Runtime Interface.Itâ€™s a **standard API** that the Kubelet uses to **talk to the container runtime** on the node â€” such as `containerd` or `CRI-O`.

      >  So, how does the Kubelet use CRI to start/stop containers?
             Hereâ€™s what happens **step by step** when the Kubelet gets instructions from the control plane:
              Example: Kubelet starting a Pod
                       1. **Kubernetes API Server** sends a pod spec (YAML) to the Kubelet.
                       2. The **Kubelet parses the pod spec**.
                       3. The Kubelet makes **gRPC API calls** (defined by the CRI) to the container runtime (like containerd), asking it to:
                             * Pull the required container image (`PullImage`)
                             * Create a sandbox for the pod (`RunPodSandbox`)
                             * Start containers (`CreateContainer`, `StartContainer`)
                       4. The container runtime **executes those instructions** and runs the containers.
            Example: Stopping Containers
                      If the pod is deleted or the container crashes, the Kubelet uses CRI calls like:
                             * StopContainer
                             * RemoveContainer
                             * RemovePodSandbox

          Diagram (Simplified):

                                  Control Plane
                                       â†“
                                  API Server
                                       â†“
                             Kubelet (on Worker Node)
                                       â†“ 
                            uses CRI (standard gRPC API)
                                       â†“ 
                          Container Runtime (containerd / CRI-O)
                                       â†“
                      Runs the actual containers on the node

Summary
| Term        | Meaning                                                                                   |
| ----------- | ----------------------------------------------------------------------------------------- |
| **Sandbox** | The isolated pod environment created by the container runtime (network, namespaces, etc.) |
|             | Only one sandbox is created per pod, no matter how many containers the pod has.           |
| **Parsing** | Kubelet reading and interpreting the pod YAML to act on it                                |

-----------------------------------------------------------------


# ======================================================================================

# ğŸ§  Kubelet â€“ Node Agent
--------------------------------------------------------------------------------------------------------

1-How does the Kubelet detect changes in PodSpecs from the API server?
2-What is a Pod Sandbox, and how does the Kubelet manage it?
3-How does the Kubelet decide when to restart a failed container?
4-What happens if the container runtime is down, but the Kubelet is running?
5-How does the Kubelet determine resource availability for scheduling pods?

-----------------------------------------------------------------

## 1. How does the Kubelet detect changes in PodSpecs from the API server?

Answer: Kubelet uses the Kubernetes API Server watch mechanism to monitor Pod changes.

Workflow:
* Kubelet registers itself with the API Server.
* It watches for PodSpecs (bound to its node) via /api/v1/pods.
* When a new or updated PodSpec is detected, Kubelet:
       - Validates it
       - Pulls container images
       - Starts containers via the Container Runtime Interface (CRI)

Follow-up:
ğŸ‘‰ â€œDoes Kubelet poll the API server?â€
No, it uses watch (via long-lived HTTP connection) for real-time updates.

-------------------------------------------------------------------------------------------------

## ğŸ§± 2. What is a Pod Sandbox, and how does the Kubelet manage it?

Answer: A Pod Sandbox is an isolated environment that Kubernetes sets up for every pod.
It handles things like network setup, hostname, IP address, and process isolation, so that containers inside the pod can communicate with each other, but are isolated from containers in other pods.
Kubernetes creates a separate sandbox for each pod, ensuring one podâ€™s environment is fully isolated from another.

Example: 
```
ğŸ¯ Pod 1: nginx-pod
Contains 2 containers: nginx , sidecar-logger

- Kubernetes creates a Pod Sandbox:
- Gives it one IP address (e.g., 10.244.0.5)
- Sets up network rules, hostname like nginx-pod
- Isolates its processes and filesystem access from other pods

â¡ï¸ Now both containers inside nginx-pod can:
- Talk to each other over localhost
- Share the same IP and port space
- See the same logs, mount same volumes, etc.

ğŸ¯ Pod 2: api-pod
Contains 1 container: python-api
- Kubernetes creates a separate Pod Sandbox:
- Gives it a different IP (e.g., 10.244.0.6)
- Own hostname like api-pod
- Separate process & network space

â¡ï¸ This pod is fully isolated from nginx-pod
- Even if they're on the same node
- It cannot access ports or files inside nginx-pod unless exposed
```

ğŸ§  Behind the Scenes:
---------------------
The Kubelet is the agent running on each Kubernetes node.
When it needs to start a pod, it doesnâ€™t directly run containers â€” it uses the CRI (Container Runtime Interface) to talk to the container runtime (like containerd or CRI-O).

```
ğŸ” KUBELET:
-------------------------------------
- Kubelet reads the Pod spec from the API server.
- It makes a "CRI call" like: " RunPodSandbox "  to the "container runtime" (e.g., containerd).
- This call:
   * Creates the network namespace
   * Assigns IP address
   * Sets up hostname, cgroups, and security context
- Once the sandbox is ready, Kubelet makes CRI calls like:
   * CreateContainer
   * StartContainer
to launch each container inside that sandbox.
```

Follow-up:
ğŸ‘‰ â€œHow do you check if the Pod sandbox is healthy?â€
Check kubectl describe pod â†’ under Events â†’ look for FailedSandbox.

-------------------------------------------------------------------------------------------------

## 3. How does the Kubelet decide when to restart a failed container?

Answer: It follows the Pod's restartPolicy (Always, OnFailure, or Never).

#### Workflow:

- Kubelet watches container status via CRI (containerd).
- On failure (non-zero exit code):
  * If restartPolicy: Always â†’ restart it immediately.
  * If OnFailure â†’ restart only on non-zero exit.
  * If Never â†’ donâ€™t restart.

#### Important:
Restart logic is Kubeletâ€™s job, not controlled by the container runtime.

#### Remember:
> The Kubelet watches pod specs using the Kubernetes Watch API mechanism â€” it doesnâ€™t poll repeatedly, but instead maintains a long-lived HTTP connection to the API Server.

> For container status, The Kubelet "polls" the container runtime (e.g. containerd, CRI-O) using the CRI interface to : 
> Check if containers are still running
> Get the status of each container (e.g. Running, Exited, CrashLoopBackOff)
> Fetch logs, exit codes, and resource usage

#### Follow-up:

* Where is the backoff delay handled?
* Kubelet applies exponential backoff to avoid rapid restarts.

* How kubelet watches containers ?

1-Kubelet communicates with the container runtime (e.g., containerd) through the CRI gRPC API.
2-It uses CRI methods like:
    * ListContainers()  *ContainerStatus()   *ListPodSandbox()
3- Kubelet does not directly "watch" container state like the API Server watch mechanism. Instead, it polls container runtime status regularly through CRI calls.
4- Based on container status (e.g., exit code, health), Kubelet takes actions like:
    *Restarting containers *Reporting to the API Server  *Updating Pod status

-------------------------------------------------------------------------------------------------

## âš ï¸ 4. What happens if the container runtime is down, but the Kubelet is running?

Answer:Kubelet cannot manage containersâ€”Pod creation, deletion, and health checks will fail.

Workflow:
    - Kubelet pings CRI socket (e.g., /run/containerd/containerd.sock)
    - If unreachable:
              * Kubelet logs errors
              * Containers can't be created/stopped
              * Node may go NotReady if health checks fail

    Follow-up:
ğŸ‘‰ â€œWill running containers keep working?â€

Yes, existing containers may keep running, but Kubelet canâ€™t monitor or restart them

-------------------------------------------------------------------------------------------------

## 5. How does the Kubelet determine resource availability for scheduling pods?

> The **Kubelet doesnâ€™t schedule pods** â€” thatâ€™s the Schedulerâ€™s job. However, the **Kubelet reports node resource status** to the **API Server**, which the Scheduler uses to make placement decisions.

 âœ… Workflow:

* **Kubelet uses `cAdvisor`** to collect **container-level metrics**, such as:

  * CPU and memory usage per container
  * Disk and network stats
* **Kubelet gathers node-level metrics** from sources like:

  * `/proc`, sysfs, cgroups, and local node introspection
* These metrics are packaged into **NodeStatus** updates and sent to the **API Server**.
* The **Scheduler** uses this data to understand:

  * Current node usage
  * Available capacity for new Pods

---

ğŸ“Œ Important:

> The Kubelet also **enforces resource limits and requests** defined in the Pod using **cgroups**. It can throttle, OOM-kill, or deny pods based on resource pressure.

---

### ğŸ” Follow-up:

> **ğŸ‘‰ Can the Kubelet reject a Pod?**
> âœ… Yes â€” if the Pod's **requested resources** exceed whatâ€™s available on the node, the **Kubelet can reject or delay** the Pod.
> Especially true during:

* CPU pressure
* Memory pressure
* Disk pressure (via eviction policies)

-----------------------------------------------------------------------------

6. **Do we need to define resource limits before a pod is scheduled to a node?**

ğŸ”¹ **No â€” it's not mandatory**, but **highly recommended**.

* If **resource `requests`** are specified:
  â¤ The **Kube-scheduler** uses them to **determine where to place** the pod.

* If **resource `limits`** are specified:
  â¤ The **Kubelet enforces** them using **cgroups** (throttling or OOM-killing if exceeded).

* If no requests are defined:
  â¤ The pod gets scheduled, but it's considered **Best-Effort**, and **may be evicted** under resource pressure.

---

### ğŸ§  Key Distinction:

| Field      | Used By          | Purpose                             |
| ---------- | ---------------- | ----------------------------------- |
| `requests` | **Scheduler**    | For pod placement decisions         |
| `limits`   | **Kubelet / OS** | For runtime enforcement via cgroups |

---

## ğŸ¯ How Interviewers May Ask This (and twist it):

 ğŸ”¸ Basic Questions:

* "What happens if you donâ€™t set resource requests or limits for a pod?"
* "Who uses resource requests â€” the Scheduler or the Kubelet?"

 ğŸ”¸ Twisted/Advanced Questions:

* â“ *â€œCan a pod with no limits get scheduled? What are the risks?â€*

  * Yes, it will be treated as **BestEffort** and **can starve other pods or get evicted**.

* â“ *â€œIf I set limits but not requests, what happens?â€*

  * Scheduler **assumes request = 0** (i.e., BestEffort), but Kubelet still **enforces the limit**. This may lead to poor scheduling decisions.

* â“ *â€œWhat if a podâ€™s request is higher than any nodeâ€™s allocatable memory?â€*

  * It **wonâ€™t be scheduled** â€” stays in **Pending**.

* â“ *â€œHow do resource limits help with multi-tenancy?â€*

  * They prevent **noisy neighbor problems**, ensure **fair usage**, and allow **QoS classification** (`Guaranteed`, `Burstable`, `BestEffort`).

* â“ *â€œCan Kubelet deny a pod even after Scheduler assigns it?â€*

  * âœ… Yes â€” during admission if node conditions are bad (e.g., memory pressure) or insufficient resources.

---

ğŸ’¡ Pro Tip to Say in Interview:

> â€œI always define both resource **requests and limits** for production pods â€” requests help the **Scheduler**, and limits ensure **runtime enforcement** by the Kubelet via cgroups.â€

---

# =====================================================================================
#   ğŸŒ Kube-Proxy â€“ Networking
----------------------------------------------------------------------------------------

# 1.  What kube proxy is ? 

 It's a networking component that runs on each node in a Kubernetes cluster.Its job is to make the Service IPs (like ClusterIP) actually work by sending traffic to the **right backend Pods**.

##### What kube-proxy does:

**Step 1: Watches for changes:**
kube-proxy is watching the Kubernetes API Server** for any changes in:
Service objects
Endpoints objects

**Step 2: Creates routing rules**
kube-proxy reads the Service IP and backend Pod IPs.
Based on your OS and kube-proxy settings, it sets up either:
iptables rules (default in most clusters), or
IPVS rules (for better performance).


-----------------------------------------------------------------------------------------------------------------------------------------------------------

##### A. How does kube-proxy manage Service IPs and route traffic to pods?


Answer: What kube-proxy is:
* It's a **networking component** that runs on **each node** in a Kubernetes cluster.
* Its job is to **make the Service IPs (like ClusterIP) actually work** by sending traffic to the right backend Pods.


##### What Happens When a Service is Created in Kubernetes (Rewritten & Corrected)

#### **1. You define a Service**

* Type: `ClusterIP`, `NodePort`, etc.
* You include a **label selector** like `app=web`.

```yaml
spec:
  selector:
    app: web
```

---

#### **2. Kubernetes (NOT CNI) assigns a ClusterIP**

* The **API Server allocates** a **virtual IP** (e.g., `10.0.0.5`) from the **Service IP range**.
* ğŸ“Œ **This is not handled by CNI** â€” CNI assigns **Pod IPs**, not Service IPs.
* This virtual IP becomes the **ClusterIP** used to access the service.

---

#### **3. Endpoint Controller creates an Endpoints object**

* It watches Services and Pods.
* If any Pods match the Serviceâ€™s label selector, it creates an `Endpoints` object:

```yaml
subsets:
  - addresses:
      - ip: 10.244.1.7
      - ip: 10.244.2.3
    ports:
      - port: 8080
```

ğŸ“Œ This object maps **Service â†’ actual Pod IPs and target ports**.

---

#### **4. kube-proxy watches Endpoints and sets up routing**

* kube-proxy runs on each node.
* It watches for changes in:

  * `Service` objects
  * `Endpoints` objects
* Depending on the mode:

  * **iptables**: writes DNAT rules
  * **IPVS**: adds backend servers to kernelâ€™s load balancer
* These rules say:
  â¤ "If traffic comes to `10.0.0.5:80`, forward it to one of the real pod IPs."

---

#### **5. Now traffic to the Service IP is load-balanced**

* From **any pod inside the cluster**, if you run:

```bash
curl http://10.0.0.5
```

* The packet is routed **by the Linux kernel**, not kube-proxy directly.
* It lands on either:

  * `10.244.1.7:8080`
  * `10.244.2.3:8080`
* Based on round-robin or IPVS scheduling algorithm.

---

##### âœ… Final Notes:

* **CNI only assigns Pod IPs**, not Service IPs.
* **Service IP (ClusterIP)** is managed internally by the Kubernetes control plane.
* **kube-proxy** only sets up routing once the **Endpoints** object is populated by the **Endpoint Controller**.


``````

WHAT IS AN ENDPOINT OVJECT IN K8S?  
- When you create a **Service** in Kubernetes (e.g., `my-service`), it uses a **label selector** to pick a set of pods that match.
- Once those pods are selected, Kubernetes **automatically creates an internal object called an `Endpoints` object** (created by endpoint controller, endpoint object service bnne k baad he bnta h ). This object holds the **real IP addresses and ports** of the matching pods.

- ğŸ’¡ Think of it like this:
> The **Service** has a **virtual IP** (`10.0.0.5`)
> The **Endpoints** object maps that virtual IP to actual **Pod IPs** and **ports** behind the scenes.

Example:

Letâ€™s say you define a Service like this:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: web
  ports:
    - port: 80
      targetPort: 8080
```

You have 2 running pods with the label `app=web`:

* Pod 1 IP: `10.244.1.7`
* Pod 2 IP: `10.244.2.3`

ğŸ” Kubernetes will create an **Endpoints** object like:

```yaml
apiVersion: v1
kind: Endpoints
metadata:
  name: my-service
subsets:
  - addresses:
      - ip: 10.244.1.7
      - ip: 10.244.2.3
    ports:
      - port: 8080
```

Now when traffic is sent to the **Service IP (`10.0.0.5`) on port 80**, kube-proxy knows to **forward it to one of the pods on port 8080** using this mapping.

---

ğŸ¯ Summary
| Component      | Purpose                                  |
| -------------- | ---------------------------------------- |
| **Service**    | Has a stable virtual IP (ClusterIP)      |
| **Endpoints**  | Maps the service to real Pod IPs + ports |
| **kube-proxy** | Handles traffic redirection using these  |

-----------------------------------------------------------------------------------------------

IS THE ENDPOINT OBEJECT CREATED ONLY WHEN SERVICE IS CREATED?
> **An `Endpoints` object is only created if a Service exists.**
> The `Endpoints` object **is created *after*** the Service is created â€” and **only if** the Service has:
     * A valid selector, and
     * There are Pods matching that selector

ğŸ” Behind the Scenes â€” Full Logic:

| Scenario                                        | Does `Endpoints` get created? | Why?                                                                              |
| ----------------------------------------------- | ----------------------------- | --------------------------------------------------------------------------------- |
| âœ… Service is created + selector matches Pods    | âœ… Yes                         | Endpoint Controller creates the `Endpoints` object listing matched Pod IPs        |
| âŒ Service is created but no Pods match selector | âœ… Yes (empty)                 | Endpoint Controller still creates the object, but with **no IPs**                 |
| âŒ No Service exists                             | âŒ No                          | There's **no reason** to create an `Endpoints` object â€” they are Service-specific |


ğŸ§  Key Rule:

> ğŸ“Œ `Endpoints` (or `EndpointSlice`) **exist only for Services**.
> They are **automatically created/updated by the Endpoint Controller** based on the Service's selector and the current set of matching Pods.

ğŸ§ª Example:

1. You define a Service with selector `app=api`:

```yaml
spec:
  selector:
    app: api
```

2. No Pods exist with `app=api` yet.

âœ… Kubernetes will still create this:

```yaml
kind: Endpoints
metadata:
  name: my-service
subsets: []    # empty â€” no Pods match
```

3. Later, a Pod with `app=api` is created

ğŸŸ¢ The Endpoint Controller detects it and **updates the Endpoints object**:

```yaml
subsets:
  - addresses:
      - ip: 10.244.3.5
    ports:
      - port: 8080
```

âœ… Conclusion:

> âœ”ï¸ **Endpoints object is tied to the Service**.
> âŒ It does **not exist independently**.
> ğŸ”„ It is dynamically created and updated **only if a Service exists**, based on its **selector** and matching **Pods**.


``````
    

### What kube-proxy does:

> **Step 1: Watches for changes:** 
* kube-proxy is watching the Kubernetes API Server** for any changes in:
* Service objects
* Endpoints objects

> **Step 2: Creates routing rules**
 * kube-proxy reads the Service IP and backend Pod IPs.
 * Based on your OS and kube-proxy settings, it sets up either:
 * iptables rules (default in most clusters), or
 * IPVS rules (for better performance).

 These rules say: 
 > "If anyone tries to access 10.0.0.5:80, forward the request to one of these real Pods: 10.244.1.7:80 or 10.244.2.3:80."

###  **What happens during a real request:?**
 

 * Letâ€™s say a pod in the cluster does: ``` curl http://10.0.0.5:80``` (service IP)
 * Behind the scenes:
 1. The **Linux networking stack** hits an **iptables rule** created by kube-proxy.
 2. This rule **redirects the request to one of the Pod IPs** randomly (load-balancing).
 3. The request **never even reaches kube-proxy as a process** â€” Linux handles it directly using the rules kube-proxy set earlier.  (IMP)
 * So kube-proxy **programs the rules**, but **doesnâ€™t touch every packet**.
 
 **How kube-proxy load-balances traffic:?**
 -------------------------------------------
1. In iptables mode: It uses round-robin to forward to different Pods.
2. In IPVS mode**: It supports advanced load balancing algorithms like:
                   * Round Robin   * Least Connections   * Source Hashing
3. You can choose IPVS by passing: ``` --proxy-mode=ipvs ```


Bonus: What are ClusterIP and Endpoints?
* **ClusterIP**: is just a **virtual IP** â€” not tied to any real interface.  It only works inside the cluster.
* The **Endpoints object** holds the **real Pod IPs** to which traffic should go.

---

 Summary (visualized):
```
User Pod (curl 10.0.0.5:80)
   â†“
Linux Networking Stack
   â†“
iptables rule (set by kube-proxy)
   â†“
Real Pod IP (e.g., 10.244.1.7:80)
```

SUMMARY 
``````
When a Service (like ClusterIP) is created in Kubernetes:
1. Kubernetes assigns a virtual IP (e.g., 10.0.0.2) to the Service â€” this is the ClusterIP.
2. The Service uses a label selector to select matching Pods.
3. Kubernetes automatically creates an Endpoints object, which lists the real Pod IPs and ports (e.g., 10.244.1.7:8080, 10.244.2.3:8080).
4. The kube-proxy watches both Services and Endpoints and, based on the current networking mode:
  > ğŸ§± iptables mode: kube-proxy writes iptables rules
  > ipvs mode: kube-proxy writes IPVS routing table entries

After that, the Linux kernel handles the actual packet forwarding â€” not kube-proxy.

ğŸ§  So in short:
kube-proxy sets up the routing logic, but it does not handle the traffic itself.
Once the rules are in place (via iptables or IPVS), the Linux kernel does all the heavy lifting of routing traffic from the Service IP to one of the Pod IPs
``````


------------------------------------------------------------------------------------------------

# ğŸ” 2. What is the difference between IPtables and IPVS modes in kube-proxy?

Both are proxy modes that kube-proxy can use to manage how traffic is routed from a Service IP to the actual Pod IPs.

ğŸ”¹ 1. IPtable mode â€“ Traditional default
-----------------------------------------

**ğŸ’¡ How it works:**

* Kube-proxy creates IPtables rules (Linux firewall rules). 
* Every time a request hits a Service IP (like 10.0.0.5), the kernel checks the IPtables rules and redirects the packet to one of the backend Pods.
* Workflow:
       Example rule: If traffic is for 10.0.0.5:80, redirect to one of: 10.244.1.7:80  10.244.2.5:80
       Load balancing is done using random or statistic match in iptables.

**ğŸ“‰ Drawbacks:**
   * Performance degrades when there are many Services and Pods because:
   * IPtables is rule-order dependent â€” it has to process one rule at a time in order.
   * Every change (new Pod/Service) causes full rebuild of the rule chain â€” time-consuming.

ğŸ”¹ 2. IPVS mode â€“ Advanced and optimized
-----------------------------------------

**ğŸ’¡ How it works:**

*  Uses Linux IPVS (IP Virtual Server), a built-in kernel-level load balancer.
*  kube-proxy programs IPVS rules using the ipvsadm tool or kernel APIs.
*  The kernel then uses a hash table, not a rule chain, for routing â€” much faster.
* Workflow: 
 Maintains a service table in the kernel:
                ``` IPVS: 10.0.0.5:80 â†’ [10.244.1.7:80, 10.244.2.5:80] ```              
Picks one Pod using one of many algorithms: Round Robin ,Least Connections, Source IP Hash

**ğŸš€ Advantages:**
*  Much faster routing, especially in large clusters (1000s of Services/Pods).
*  Rule updates are incremental â€” only changes are applied, not full table rewrite.

**ğŸ” Side-by-side comparison:**
| Feature         | **iptables**                           | **IPVS**                                |
| --------------- | -------------------------------------- | --------------------------------------- |
| **Mode**        | Sequential rule matching (`netfilter`) | Kernel-based virtual server             |
| **Performance** | Slower with scale                      | Faster, especially in large clusters    |
| **Rule Mgmt**   | Full table rebuild on updates          | Only affected entries updated           |
| **Algorithms**  | Only basic round-robin/statistic       | Many (Round Robin, Least Conn, etc.)    |
| **Memory use**  | Lighter, good for small clusters       | Slightly higher but scalable            |
| **Stability**   | Stable, default in most clusters       | Stable but needs IPVS modules installed |

**ğŸ§  Key Point:**
  * Use iptables in small/simple clusters.
  * Prefer IPVS in production or large-scale clusters for: Faster failovers , Scalability , Advanced load balancing

**ğŸ› ï¸ How to enable IPVS mode:** 
         In your kube-proxy config or DaemonSet:
``` 
command:
  - kube-proxy
  - --proxy-mode=ipvs
```
Also ensure the node has IPVS kernel modules:
``` lsmod | grep ip_vs ```

------------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”„ 3. How does kube-proxy handle updates to Service or Endpoints objects?

Answer: Kube-proxy watches the Kubernetes API Server for changes to Service and Endpoints objects, and updates its routing rules (iptables or IPVS) accordingly â€” all this happens automatically and with minimal traffic disruption.

ğŸ§  What triggers an update?
--------------------------

* When something changes like: A new Pod is created or deleted,  A Service selector is updated, A Pod becomes Ready/Unready (so its IP should be added/removed).
* This causes the Endpoints object to change, and that triggers kube-proxy to act.

âš™ï¸ Behind the scenes: Workflow
---------------------------------

1- ğŸ•µï¸â€â™‚ï¸ kube-proxy is continuously watching the API Server (using a Watch API). It monitors Service and Endpoints objects.

2- ğŸ”„ If an update is detected:  For example: a Pod is deleted â†’ the Endpoints list is now shorter.

3- ğŸ” kube-proxy compares the old state and the new state of these objects.
  - What was added? 
  - What was removed? 
  - What changed?

4- ğŸ› ï¸ Based on the difference, it updates the networking rules:
   - If using iptables: it rebuilds the relevant rules.
   - If using IPVS: it incrementally updates the internal routing tables.

5- ğŸš€ These changes take effect immediately, allowing traffic to be routed to the new set of healthy Pod IPs.

ğŸ“¦ Summary:
| Step            | What Happens                               |
| --------------- | ------------------------------------------ |
| Watch           | kube-proxy watches API Server for changes  |
| Detect          | Notices change in Service or Endpoints     |
| Compare         | Old vs new configuration                   |
| Update Rules    | Updates iptables/IPVS with correct Pod IPs |
| Forward Traffic | Keeps traffic flowing to valid Pods only   |

Follow-up:
ğŸ‘‰ â€œIs the traffic affected during update?â€
Minimal or no downtime if kube-proxy is healthy.

------------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”„ 4. Can kube-proxy be replaced or customized? (e.g., Cilium)

Answer: Yes, kube-proxy can be replaced by eBPF-based networking solutions like Cilium.
Common Alternatives:
> Cilium: Uses eBPF, replaces kube-proxy for high-performance routing.
> Calico, Kube-router: May also replace kube-proxy functionality.

Why replace it?
 - Better performance
 - Better observability
 - Native support for Network Policies

Follow-up:
ğŸ‘‰ â€œWhat is eBPF?â€
Extended Berkeley Packet Filter â€“ allows fast packet processing in the Linux kernel without modifying kernel code.

------------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ§¹ 5. What happens in kube-proxy if a pod behind a service is deleted?

Answer:  When a Pod is deleted, the corresponding **"Endpoints object"** for the Service is updated, and kube-proxy removes that Podâ€™s IP from its routing rules â€” so traffic is no longer routed to the deleted Pod.

âš™ï¸ Workflow (Step-by-step):
--------------------------

  1- âŒ A Pod (say 10.244.2.5) is deleted (manually or due to a crash).

  2- ğŸ§  The Kubernetes Endpoint controller detects this change. It updates the Endpoints object of the Service to remove 10.244.2.5.
  
  3- ğŸ•µï¸â€â™‚ï¸ kube-proxy is watching for these updates via the API Server (watch)

  4- ğŸ”„ When it notices the Endpoints list has changed: > It updates iptables or IPVS rules. > Removes the rule forwarding to the deleted Pod.

  5- ğŸš« Now, traffic going to the Service IP will not be forwarded to the dead Pod.

  6- âœ”ï¸ Users/Clients wonâ€™t hit the deleted Pod anymore â€” itâ€™s cleanly removed from routing.

ğŸ” Behind the scenes (technical flow):
   * The Podâ€™s IP is removed from: Endpoints object of the Service (handled by Kube controller manager).
   * This triggers an event, which kube-proxy is subscribed to via a watch.
   * Based on your proxy mode:
          - iptables: the rules redirecting traffic to the Pod IP are removed.
          - IPVS: the backend server entry is deleted from the kernelâ€™s load balancer.

â“ Follow-up 

Q: "Is there a delay in removal? 
Answer: Usually the delay is a few seconds or less â€” kube-proxy is fast.
But delays can happen if:
   - The API Server is under load.
   - kube-proxy is slow to process events.
   - The Pod termination signal is delayed (e.g., in a crash or node failure).
To reduce risk of stale traffic:
   - Use readiness/liveness probes to mark Pods "unready" before deletion.
   - Consider using graceful termination periods in deployments.

------------------------------------------------------------------------------------------------------------------------------------------------------------

# =========================================================================================================================================================
##   ğŸ“¦ API Server â€“ Cluster Gateway 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
# 1- ğŸ“¡ How does the API server ensure high availability and leader election? How does Kubernetes make sure the API server stays up and available even if one instance goes down?

The API Server is made highly available by running multiple replicas behind a load balancer. The leader election is relevant for certain control plane components (like controller-manager or scheduler), not the API server itself â€” since all API server replicas are active, not just one.

ğŸ§± High Availability (HA) of API Server: âš™ï¸ Workflow:
----------------------------------------------------

1- You deploy multiple API server pods or instances (e.g., 3).

2- These are placed behind a load balancer (like AWS ELB or HAProxy).

3- Clients (like kubectl, kubelet, controllers) send requests to the load balancer.

4- The load balancer distributes traffic among healthy API servers.

5- All replicas are stateless â€” they store state in etcd, not locally.

ğŸ“Œ Important: API servers do not need leader election because they serve requests in parallel.

> ğŸ¤ Leader Election â€“ Applies to Controller Manager and Scheduler
For components like kube-controller-manager and kube-scheduler, only one instance should be active at a time, even if multiple replicas are running.

ğŸ’¡ How it works:

- All controller-manager/scheduler replicas try to become leader.
- They attempt to acquire a lease lock (usually stored in a Kubernetes ConfigMap or Lease object).
- One becomes the leader; others stay in standby mode.
- If the leader goes down: Others detect the lost lease.A new leader is elected automatically.
This is handled using Kubernetes leader-election library (based on distributed coordination via etcd).

ğŸ” Behind the Scenes (Leader Election):
- Lease object stored in Kubernetes API (e.g., /kube-system/leader-election).
- Uses etcd as the backend data store.
- Election happens by writing to the lease object:
      * First one to claim the lease wins.
      * Periodic renewal ensures leader is still alive.
      * Timeout â†’ others can take over.

     ğŸ§ª Example Use Case:
   Letâ€™s say 2 replicas of kube-controller-manager are running: Replica 1 becomes leader (writes lease).It manages deployments, replicasets, etc.Replica 2 does nothing â€” 
   just waits.If replica 1 dies, replica 2 sees lease timeout â†’ takes over.

â“ Follow-Up Questions You Might Be Asked:
--------------------------------------------
Q: Why does the API server not require leader election?

A: Because all API server instances are stateless and can serve traffic concurrently.

Q: What happens if etcd goes down?

A: The API servers will not function fully â€” they canâ€™t store or retrieve cluster state.

Q: How is the health of API servers checked?

A: The load balancer uses health checks (e.g., /healthz) to route traffic only to healthy servers.

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 2 - â“ What are Admission Controllers?

They are checkpoints inside the API server that act like gatekeepers for any create, update, delete requests in Kubernetes.

ğŸ“¦ Imagine this workflow:
--------------------------

> You run:  ```kubectl create pod mypod.yaml```

> Hereâ€™s what happens step-by-step:
   * âœ… Authentication â€“ Are you a valid user?
   * âœ… Authorization â€“ Are you allowed to create a Pod?
   * ğŸ›‘ Admission Controllers â€“ Should we allow or reject this Pod based on: Rules, Security, Custom policies, Auto-editing (e.g., adding labels)

ğŸ’¾ If allowed â†’ object is saved into etcd (the cluster database).

``````
ğŸ§  Example Scenario:
Letâ€™s say a developer tries to create a Pod without memory limits.

A. âœ… AuthN: The user is authenticated (valid token).
B. âœ… AuthZ: The user is authorized to create Pods.
C. ğŸ›‚ Admission Controller kicks in:
* LimitRanger sees no memory limits â†’ rejects the Pod creation with:
"must specify memory limits"
D. Nothing is stored in etcd â€” the pod never makes it into the cluster.
``````

ğŸ” Common Admission Controllers:
---------------------------------------

| Controller Name              | What it does                               |
| ---------------------------- | ------------------------------------------ |
| `NamespaceLifecycle`         | Blocks resources in terminating namespaces |
| `LimitRanger`                | Enforces resource limits                   |
| `PodSecurity`                | Blocks insecure Pods                       |
| `MutatingAdmissionWebhook`   | Changes the object (e.g., injects sidecar) |
| `ValidatingAdmissionWebhook` | Validates but doesn't modify               |

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 3. ğŸ” What is Authentication vs Authorization in the API Server?

âœ… Short Summary:
--------------------

| Term                       | Purpose                                                   |
| -------------------------- | --------------------------------------------------------- |
| **Authentication (AuthN)** | Proves **who you are** (identity check)                   |
| **Authorization (AuthZ)**  | Decides **what you're allowed to do** (permissions check) |

 âš™ï¸ Full Request Processing Workflow (Behind the Scenes)
 ------------------------------------------------------------

When a client (like `kubectl`) sends a request to the **Kubernetes API server**, hereâ€™s what happens:

### ğŸ§­ Step-by-step:

1. âœ… **Authentication**

   * First, the API server checks the **identity of the client**.
   * It asks:

     > *"Is this certificate/token/password valid?"*
   * If invalid, the request is rejected with **401 Unauthorized**.

2. âœ… **Authorization**

   * If authentication passes, now the server checks:

     > *"Is this user allowed to perform this action on this resource?"*
   * If the action is not permitted, request is rejected with **403 Forbidden**.

3. âœ… **Admission Controllers**

   * If both AuthN and AuthZ pass, the request goes to **Admission Controllers** (for policy checks like quotas, required labels, etc.)

4. âœ… **Persistence**

   * If everything passes, the request is processed and changes (if any) are stored in **etcd** (the backing data store of the cluster).

---

### ğŸ›¡ï¸ **Authentication Methods (Who are you?)**

| Method                  | Description                                                          |
| ----------------------- | -------------------------------------------------------------------- |
| **Client Certificates** | Common in kubeletâ€“API Server communication                           |
| **Bearer Tokens**       | ServiceAccounts or static secrets                                    |
| **OIDC (OAuth2)**       | For enterprise SSO integration                                       |
| **Static Files**        | Passwords or tokens from flat files (not recommended for production) |

---

### ğŸ›‚ **Authorization Modes (Can you do this?)**

| Mode                         | Description                                                              |
| -----------------------------| ------------------------------------------------------------------------ |
| âœ… **RBAC** (most common)    | Permissions based on roles and bindings (e.g., allow "devs" to get pods) |
| **ABAC (Attribute-Based)**   | JSON policies â€” deprecated and not dynamic                               |
| **Node Authorization**       | Special mode for kubelet to access only its own resources                |
| **Webhook Authorization**    | External system makes the decision via REST call                         |

> ğŸš¦ The API Server can be configured with **multiple authZ modes enabled**. It evaluates them **in order**, and the **first definitive answer** (Allow or Deny) is used.

---

 â“ Interview Follow-Up Q\&A
--------------------------------
ğŸ”¸ Q: *What happens if authentication passes but authorization fails?*
âœ… Answer:
> The API server returns **403 Forbidden** â€” the user is known, but **not allowed** to do the requested action.

---

ğŸ”¸ Q: *Can multiple authorization modes be used together?*

> Yes. The API Server evaluates them **in the order configured**.
> It uses the **first mode** that returns a decision (`Allow` or `Deny`).
> If all modes **abstain**, the request is denied by default.

ğŸ”¸ Q: *Who manages RBAC policies?*

> RBAC roles and bindings are managed via Kubernetes YAML or `kubectl`, e.g.:

```bash
kubectl create rolebinding view-pods \
  --role=view \
  --user=alice \
  --namespace=dev
```

âœ… Final Analogy:

> Think of **Authentication** as checking your **ID at the door**, and **Authorization** as checking if you have **access to the room youâ€™re trying to enter**.



-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 4. ğŸš¦ How does the API server handle excessive request loads (rate limiting)?

The Kubernetes API Server protects itself from overload by using rate limiting, priority levels, and concurrency controls. This ensures fair usage and prevents any user or component from overwhelming the server.

âš™ï¸ Mechanisms Used:
--------------------

1. Client-side Rate Limiting via kubelet, kubectl, etc.:
 Clients like kubectl or kubelet have built-in limits: Example: default 5 QPS (queries per second) and 10 burst for kubelet. You can configure it:
 ``` kubectl --request-timeout=30s --v=9 ```

2. API Server Priority and Fairness (APF): 
* Since Kubernetes v1.20+, the API server uses Priority and Fairness (APF).
* It divides traffic into **â€œpriority levelsâ€** and **â€œflowsâ€**, like: 
    ``````> System components (high priority) > user requests (medium)  > Background jobs (low)``````
* ğŸ” Each flow gets a fair share of API server resources. If a request exceeds its limit: It is either queued or rejected with 429 Too Many Requests.

ğŸ” Example Flow:
------------------

 > You have a script sending 1000 pod creation requests.
 > API server groups those requests into a â€œflowâ€.
 > If that flow is over its fair share, Kubernetes:
       * Queues some requests temporarily.
       * Drops or delays others to avoid overloading etcd and the API server.


â“ Follow-up Questions You May Get:
----------------------------------------

Q: What happens when the API server gets a lot of requests at once?
A: It queues or throttles requests based on priority/fairness.

Q: What status code is returned when throttled?
A: HTTP 429 Too Many Requests.

Q: How can you control a componentâ€™s rate?
A: By adjusting client flags like --kube-api-qps and --kube-api-burst.

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 5. Whatâ€™s the role of ETCD in response to an API request?

ETCD is the backend database of Kubernetes. It stores the entire cluster state â€” all objects like Pods, Services, ConfigMaps, Secrets, etc. When an API request reads or writes data, the API server interacts with ETCD to fetch or store that state.

âš™ï¸ Workflow (Behind the Scenes):
---------------------------------
Letâ€™s say you run ```kubectl create pod mypod.yaml```  Hereâ€™s what happens: 
- ğŸ” Authentication & Authorization    
- âœ… Admission Controllers    
- ğŸ’¾ etcd Write: API server stores the Pod object into etcd. etcd makes it durable, consistent, and replicated

Now, if you run ```kubectl get pods``` , API server reads the pod list from etcd , Returns it to you via kubectl

ğŸ” ETCD Ensures:
| Feature               | Role                                      |
| --------------------- | ----------------------------------------- |
| **Consistency**       | All API servers get the same latest state |
| **High Availability** | Replicated across nodes                   |
| **Durability**        | Data survives restarts and failures       |

â“ Follow-up Questions You Might Be Asked:
--------------------------------------------

Q: What happens if ETCD goes down?
A: The cluster becomes read-only or unstable â€” new objects canâ€™t be stored, and existing state may be outdated.

Q: Is data cached somewhere?
A: Yes, the API server may cache some objects in memory for performance, but ETCD is the source of truth.

Q: What kind of database is ETCD?
A: Itâ€™s a distributed key-value store, built on the Raft consensus algorithm for strong consistency.

# =========================================================================================================================================================

# ğŸ” ETCD â€“ Key-Value Store
-----------------------------------------------------------------------------------------------------------------------------------------------------------
# 1. ğŸ“¦ What exactly does ETCD store? Can you give examples of keys?

ETCD stores all cluster state as key-value pairs â€” this includes everything the Kubernetes API knows.

ğŸ” Examples of what ETCD stores:

| Kubernetes Resource | etcd Stores                  |
| ------------------- | ---------------------------- |
| Pods                | Spec, status, metadata       |
| Nodes               | Node info and heartbeats     |
| ConfigMaps          | Config data                  |
| Secrets             | Base64-encoded secret values |
| ServiceAccounts     | Tokens, metadata             |
| Deployments, RS     | Their spec and relationships |
| Events, RBAC rules  | Audit and permission data    |

ğŸ—ï¸ Examples of actual etcd keys:
These keys look like file paths:
- /registry/pods/default/my-app-pod
- /registry/nodes/node-1
- /registry/secrets/default/db-password
- /registry/deployments/default/my-deployment
ğŸ‘‰ The value stored under each key is the full object in JSON or protobuf format.

â“ Follow-up You Might Get:
-----------------------------

Q: Can I read etcd directly?
> A: Yes, but you should never manually edit etcd â€” always use kubectl or the API server.

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 2- ğŸ’¥ What are the risks of a single ETCD node failure?

If your ETCD has only one node and it fails, the entire cluster becomes unstable.

| ğŸ§© **Risk Type**         | ğŸ’¥ **What Happens**                                                               |
| ------------------------ | --------------------------------------------------------------------------------- |
| âŒ **No Writes**          | API Server can't store any changes (e.g., creating new Pods, updates, scaling)    |
| âš ï¸ **Read Instability**  | May serve stale or inconsistent data â€” not guaranteed to reflect the latest state |
| ğŸ”„ **Controllers Break** | Auto-scaling, self-healing, and rolling deployments may **fail to trigger**       |
| ğŸ§¨ **No Recovery**       | If the disk is lost and thereâ€™s no backup, the **cluster state is lost forever**  |


ğŸ’¡ Best Practice:
------------------

* Run etcd as a 3-node cluster (or 5 in production) for:
* High availability  
* Quorum-based consistency (Raft)
* ğŸ‘‰ etcd needs a majority quorum (e.g., 2 of 3) to work.

â“ Follow-up You Might Be Asked:
----------------------------------

Q: Whatâ€™s the minimum number of etcd nodes for high availability?
A: 3 (majority is 2).

Q: What if 2 out of 3 etcd nodes fail?
A: Cluster becomes unavailable for writes (no quorum).

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 3- ğŸ’¾ How do you take and restore a backup of ETCD?

* **Use the **etcdctl** snapshot command to take a backup and restore it later into a new etcd cluster or Node.** 
* **To Take a Backup (Snapshot):**

```
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /tmp/etcd-backup.db
```
âœ… This saves a point-in-time copy of the etcd database to /tmp/etcd-backup.db.

* ğŸ› ï¸ **To Restore a Backup**

```
  ETCDCTL_API=3 etcdctl snapshot restore /tmp/etcd-backup.db \
  --data-dir /var/lib/etcd-restore
```
* Then use the restored --data-dir in your etcd startup config.

ğŸ” **Important Notes:**
-------------------------

- Always take backups periodically (daily or more).
- Back up before upgrades or config changes.
- Store snapshots off-node (e.g., S3, NFS, etc.).

â“ **Follow-up Interview Questions:**
-----------------------------------

Q: Can you automate etcd backup?
A: Yes â€” use a cronjob or Kubernetes Job + PV to back up regularly.

Q: Will restoring etcd restore everything in the cluster?
A: Yes â€” etcd contains all cluster objects. But it wonâ€™t restore persistent volumes unless backed up separately.

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 4-  ğŸ¢ What is the impact of ETCD latency on the whole cluster?

High etcd latency slows down API operations, controller decisions, and scheduling â€” impacting the entire clusterâ€™s responsiveness and stability.

âš ï¸ What Happens When etcd Is Slow:
-----------------------------------

| Area Affected                 | What You See                             |
| ----------------------------- | ---------------------------------------- |
| `kubectl` commands            | Slow or time out (e.g., `kubectl get`)   |
| Pod scheduling                | Scheduler can't read/write pod info fast |
| Controllers (e.g. Deployment) | Slow to create/update/delete pods        |
| Autoscaling & healing         | Delays in decision making                |
| Cert expiration               | May fail if renewal data canâ€™t be stored |

ğŸ” Root Causes of Latency:ğŸ”§ How to Monitor and Fix:
---------------------------------------------------

Monitor metrics like:
- etcd_disk_wal_fsync_duration_seconds
- etcd_server_has_leader (should always be 1)
- Use fast SSDs or dedicated nodes for etcd
- Keep etcd data size under control (avoid many Events or large Secrets)

â“ Follow-up Interview Questions:
-----------------------------------

Q: Whatâ€™s the recommended storage type for etcd?
> A: Fast SSDs, ideally on dedicated nodes.

Q: How can you reduce etcd load?
> A: Clean up old Events, donâ€™t store large config/secrets, use kube-apiserver caching.

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 5- ğŸ” How does Kubernetes ensure consistency and availability in ETCD reads/writes?

- Kubernetes relies on etcd, which uses the Raft consensus algorithm to ensure:
- Strong consistency (only one agreed-upon version of data) , 
- High availability (even if some etcd nodes fail)

 âš™ï¸ Workflow: etcd Write Operation
--------------------------------------
> Example: `kubectl create -f mypod.yaml`

## ğŸ”„ Step-by-Step:

ğŸ§¾ Step 1: kubectl create -f mypod.yaml
- ou make a write request (e.g. creating a Pod)

ğŸ§  Step 2: API Server handles the request
- Authentication: Who are you?
- Authorization: Are you allowed to do this?
- Admission Controllers: Any rules to apply?
- âœ… If all good â†’ prepares the object and sends it as a write request to etcd

ğŸ“¡ Step 3: etcd (Raft Protocol)
- Only the etcd leader can handle writes.
- The leader sends the proposed change to follower nodes.
- Followers vote on it.

ğŸ“Š Step 4: Quorum and Commit
- If a majority (quorum) of nodes agree (e.g., 2 out of 3):
- The leader commits the change to its own log
- Then followers apply the committed change
- Only then does the API server get a âœ… success response

ğŸ’¡ Why this matters:
- Ensures strong consistency: no node accepts different versions of the same data
- Ensures availability: even if one node is down, 2 out of 3 is enough for quorum
> ğŸ” This guarantees **strong consistency** â€” no changes are considered â€œcommittedâ€ unless the majority of etcd nodes agree.


ğŸ” Workflow: etcd Read Operation
-----------------------------------
1. ğŸ‘¤ You make a request
Example: kubectl get pod mypod

2. ğŸ§  API Server handles the request
- Performs Authentication
- Performs Authorization
- Checks cache (in-memory) for the object
- If cache is not fresh or unavailable, it goes to etcd

3. ğŸ“– etcd handles the read
- The API Server connects to any etcd node (not necessarily the leader)
- The etcd node may:
  * Return the value from its local state (fast, but might be stale), or
  * Forward the read to the etcd leader for a linearizable (strongly consistent) rea

> Example: `kubectl get pods`

ğŸ” Two Types of Reads in etcd


| Read Type                                       | How it works                                                                     | Guarantees                                                            | Use Case                           |
| ----------------------------------------------- | -------------------------------------------------------------------------------- | --------------------------------------------------------------------- | ---------------------------------- |
| **Default (linearizable)**                      | Reads from etcd **leaderâ€™s local memory** (cache)                                | Fast & often consistent â€” but not guaranteed to have the latest write | Most kubectl queries               |
| **Strong Consistency (`--consistency=strong`)** | Forces etcd to check with **quorum (majority of nodes)** before serving the read | âœ… Guaranteed latest committed value                                   | Read-after-write or critical reads |


```bash
kubectl get pods --consistency=strong
```

> âœ… This ensures you're reading the **latest committed state** across the cluster.




âœ… Example

A. Imagine this situation: You run: `````` kubectl create pod mypod.yaml `````` This writes to etcd.
B. Immediately after, you run: ```kubectl get pod mypod``` . If you donâ€™t use --consistency=strong, the leader may serve a stale read from memory (before quorum finished committing).
C.If you do use --consistency=strong, the read will wait until the value is confirmed committed across quorum â€” so itâ€™s guaranteed fresh.
> --consistency=strong tells etcd: "Don't just give me what you have in memory â€” first make sure majority of your cluster agrees this value was committed. Then give it to me."



ğŸ§  Why This Matters:
---------------------

* **Writes require quorum** â†’ guarantees high durability and consistency
* **Reads can be fast (default)** or **fully consistent (with quorum)** depending on the use case
* A **write never succeeds unless persisted to multiple nodes**, making etcd fault-tolerant


ğŸ§  How Raft Ensures Consistency:
----------------------------------
| Concept       | Role                                  |
| ------------- | ------------------------------------- |
| **Leader**    | Accepts and coordinates writes        |
| **Followers** | Replicate data from leader            |
| **Quorum**    | Majority needed to commit a write     |
| **Term**      | Time period where a leader is elected |
âš ï¸ If the leader fails, a new leader is elected automatically â€” so etcd stays available.

ğŸ“ˆ High Availability (HA) Strategy:
-------------------------------------
- Always use an odd number of etcd nodes (e.g., 3 or 5)
- Quorum = (N/2) + 1
Without quorum, writes will stop, but reads may continue

â“ Possible Interview Follow-ups:
------------------------------------
Q: What happens if etcd loses quorum?
A: No writes can be made; the cluster becomes read-only or degraded.
Q: **What ensures consistency in etcd?**
A: **The Raft protocol, which replicates logs and requires majority consensus.**
Q: Can you improve etcd read performance?
A: Yes â€” by using API server caching, and not forcing strong reads unless necessary.

# =========================================================================================================================================================

# ğŸ“… Scheduler
-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 1- ğŸ“¦ How does the Kubernetes Scheduler choose a node for a pod?

The Kube-scheduler selects the best node for a Pod by running a two-step process:
 - **Filtering** â†’ **Eliminate** nodes that can't run the Pod
 - **Scoring** â†’ **Rank** the remaining nodes and pick the best one

Here's a **clean, detailed, and interview-friendly rewrite** of your **Kubernetes Scheduler Workflow**, with clarifications, improved structure, and explanations for each step:

---

âš™ï¸ Pod Scheduling Workflow (Behind the Scenes)
------------------------------------------------

ğŸ” Step-by-Step:

1. ğŸ†• **Pod is Created (Unscheduled)**

* A new Pod is created without a `nodeName`.
* Example:

  ```yaml
  spec:
    nodeName: null  # Unassigned
  ```
---

2. ğŸ“¡ **API Server Adds the Pod to the Scheduling Queue**

* Since the Pod is unscheduled, itâ€™s placed in the **pending scheduling queue**.
* The API server doesnâ€™t assign a node â€” it waits for the **Scheduler** to do that.

---

3. ğŸ§  **Scheduler Detects the Pending Pod**

* The **Kube Scheduler** continuously watches for **unscheduled Pods** via a **watch mechanism** on the API server.
* It picks up the Pod for processing.

---

4. ğŸ§ª **Scheduler Runs Filter Phase (a.k.a. "Predicates")**

* It eliminates nodes that **cannot** run the Pod based on constraints:

  | Predicate Check              | Description                                                      |
  | ---------------------------- | ---------------------------------------------------------------- |
  | âœ… **Resource Fit**           | Does the node have enough **CPU/Memory**?                        |
  | âœ… **Node Affinity / Taints** | Does the Pod tolerate the node's taints or match affinity rules? |
  | âœ… **Node Readiness**         | Is the node `Ready` and schedulable?                             |
  | âœ… **Port Conflicts**         | Are the ports required by the Pod already in use?                |

* After filtering, only **eligible nodes** remain.

---

5. ğŸ§® **Scheduler Runs Scoring Phase (a.k.a. "Priorities")**

* Assigns scores to remaining nodes to find the **best fit**:

  | Priority Function             | Description                                 |
  | ----------------------------- | ------------------------------------------- |
  | ğŸ”„ **PodTopologySpread**      | Spread Pods evenly across zones/nodes       |
  | ğŸ§® **LeastRequestedPriority** | Prefer nodes with more free resources       |
  | ğŸ“Œ **Node Affinity Priority** | Give higher score to nodes preferred by Pod |
  | âš¡ **Custom Plugins**          | If enabled, score based on external rules   |

---

6. âœ… **Scheduler Picks the Highest-Scoring Node**

* After scoring, the scheduler selects the **node with the highest score** to assign the Pod.

---

7. ğŸ§· **Scheduler Patches the Podâ€™s `nodeName`**

* It makes an API call to **bind the Pod**:

```yaml
spec:
  nodeName: chosen-node
```

* This officially **assigns the Pod** to a node.

---

8. ğŸ” **Kubelet on the Target Node Now Takes Over**

* The Kubelet on `chosen-node`:

  * Sees the assigned Pod
  * Pulls container images
  * Creates sandbox and containers
  * Starts the Pod

---

### âœ… Summary (One-Liner Style):

> The Scheduler filters nodes based on constraints, scores the rest, picks the best node, and binds the Pod to it by setting `spec.nodeName`.

---

â“ Follow-up Questions:
----------------------------
Q: Who actually starts the Pod?
> A: The kubelet on the assigned node.

Q: What if all nodes fail filters?
> A: Pod stays unscheduled until conditions change.

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 2 - ğŸ§® What are Predicates and Priorities (now Filter and Score)?

These were renamed in Kubernetes v1.19+:
| Old Term (v1.18 and below) | New Term (v1.19+) |
| -------------------------- | ----------------- |
| **Predicate**              | **Filter plugin** |
| **Priority**               | **Score plugin**  |

ğŸ§ª Filter Plugins (Predicates):
---------------------------------
These eliminate non-eligible nodes:
* NodeUnschedulable (cordoned node)
* PodFitsResources (CPU/RAM not enough)
* PodToleratesNodeTaints
* NodeAffinity / NodeSelector
ğŸ” Think: "Can this node run the pod?"

ğŸ“Š Score Plugins (Priorities):
---------------------------------
These rank eligible nodes:
* LeastRequestedPriority (prefer low usage)
* BalancedAllocation (CPU/mem balance)
* TopologySpreadConstraint (spread across zones/racks)
* NodeAffinityPriority (custom rules)
ğŸ” Think: "Which of these eligible nodes is best?"

â“ Follow-up Questions:
-------------------------------
Q: Can you customize this behavior?
> A: Yes, via scheduler profiles or custom plugins.

Q: What plugin would spread pods across zones?
> A: PodTopologySpread scoring plugin.

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 3 - ğŸš«  How do Taints and Tolerations influence scheduling decisions?

**Taints** are **set on nodes** to repel Pods. **Tolerations** are added to **Pods** to allow scheduling on tainted nodes. A Pod must tolerate all taints on a node to be scheduled there.

âš™ï¸ Workflow:
----------------

- A node is tainted, for example: ```kubectl taint nodes node1 key=value:NoSchedule``` 
- The scheduler sees this taint. 
- If a Pod has a matching toleration, it's allowed to be scheduled. 
- If not, it's filtered out during scheduling.

ğŸ§  Use Case:
--------------
- Dedicate nodes for GPU or system workloads.
- Prevent general Pods from landing on sensitive nodes.

â“ Follow-up Questions:
------------------------
Q: What are the taint effects?
> A: NoSchedule, PreferNoSchedule, NoExecute

Q: What happens with NoExecute?
> A: Evicts running Pods without a matching toleration.

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 4. What happens when no node fits the podâ€™s constraints?

- The Pod remains Pending until a suitable node becomes available.
- Common reasons: 
   * Insufficient CPU/RAM , 
   * Taints without matching tolerations , 
   * Node affinity rules not met ,  
   * Volume limits exceeded , 
   * Topology spread constraints too strict

â“ Follow-up Questions:
----------------------------
Q: How do you debug a Pending Pod?
> A: Use kubectl describe pod and check Events section.

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 5. ğŸ”„  How do Affinity / Anti-affinity rules influence scheduling?

They guide the scheduler to prefer or require that Pods are co-located or spread across nodes based on labels.

ğŸ’¡ Types:
-----------
| Type                  | Purpose                         | Example                            |
| --------------------- | ------------------------------- | ---------------------------------- |
| **Node Affinity**     | Schedule Pods on specific nodes | Based on node labels               |
| **Pod Affinity**      | Place Pods together             | E.g. web + cache on same node      |
| **Pod Anti-affinity** | Spread Pods apart               | E.g. avoid single point of failure |

âš™ï¸ Workflow Example (Pod Anti-affinity):
----------------------------------------
```
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app: web
        topologyKey: "kubernetes.io/hostname"
```
> â¡ï¸ This rule prevents multiple web Pods on the same node.

â“ Follow-up Questions:
--------------------------
Q: Whatâ€™s the difference between preferred and required?
> A: required is mandatory (hard rule); preferred is best effort (soft rule).

Q: What is topologyKey?
> A: It's the label like kubernetes.io/hostname or topology.kubernetes.io/zone used to determine groupings for affinit

# =========================================================================================================================================================
ğŸ”„ Controller Manager
-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 1- ğŸ§  What are the types of controllers running in the Controller Manager?

The Controller Manager runs multiple built-in controllers that monitor the cluster state and reconcile it to the desired state.

âš™ï¸ Common Controllers:
-----------------------
| Controller                     | Purpose                                          |
| ------------------------------ | ------------------------------------------------ |
| **Deployment Controller**      | Manages ReplicaSets for Deployments              |
| **ReplicaSet Controller**      | Ensures correct number of pod replicas           |
| **StatefulSet Controller**     | Manages unique identity pods with stable storage |
| **DaemonSet Controller**       | Ensures one Pod per node                         |
| **Job/CronJob Controllers**    | Run batch or scheduled jobs                      |
| **Node Controller**            | Tracks node status and handles node failures     |
| **EndpointSlice Controller**   | Manages network endpoint objects                 |
| **Service Account Controller** | Creates default service accounts                 |
| **Namespace Controller**       | Cleans up resources in deleted namespaces        |
| **PV/PVC Controllers**         | Manages volume provisioning and binding          |

ğŸ”„ How They Work:
-------------------
- Each controller uses the Informer pattern:
- Watches objects (e.g., Deployments, Nodes)
- Compares actual vs desired state
- Issues API calls to fix the drift (e.g., creating/deleting pods)

â“ Follow-up Questions:
------------------------
Q: Are these separate processes?
> A: No, they run as goroutines inside the same controller-manager process.

Q: Can you add custom controllers?
> A: Yes, via Operators or custom controllers using client-go.

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 2- ğŸš€  How does the Deployment controller ensure a rollout happens correctly?

The Deployment Controller manages rollouts by creating and updating ReplicaSets, ensuring Pods are updated gradually and safely.

âš™ï¸ Workflow of a Rollout:
-------------------------
- You apply a new Deployment  ``` kubectl apply -f deployment.yaml ```
- The Deployment controller:
   * Detects the change
   * Creates a new ReplicaSet for the new version
   * Scales up the new ReplicaSet and scales down the old ReplicaSet
- It uses the strategy defined in the Deployment:
   * RollingUpdate (default)
   * Recreate (delete all, then create new)
- It honors parameters like:
   * maxUnavailable: how many old Pods can be down during update
   * maxSurge: how many extra new Pods can be created temporarily

ğŸ’¬ Example:
------------
```
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 1
    maxSurge: 2
```
ğŸ§  Behind the Scenes:
---------------------------
- Uses ReplicaSet controller to handle pod scaling
- Ensures new Pods are Ready before proceeding to next batch
- If a failure is detected, rollout can be paused or rolled back

â“ Follow-up Questions:
-----------------------
Q: How does the controller detect a rollout failure?
> A: Based on readiness probes and Pod status, it monitors availability.

Q: How do you pause or undo a rollout?
A:
```
kubectl rollout pause deployment <name>
kubectl rollout undo deployment <name>
```

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 3- ğŸ§¨  What happens if the ReplicaSet controller fails?

If the ReplicaSet controller fails:
- No new Pods will be created or deleted for that ReplicaSet.
- The system becomes stale â€” current state will not be reconciled with desired state.

ğŸ§  Important Notes:
- Existing Pods will keep running, since kubelet manages them on the nodes.
- Other controllers (e.g., Deployment controller) may still function.
- Once the controller comes back up, it resumes reconciliation using event-based watches and local cache.

â“ Follow-up Questions:
--------------------------

Q: Is there a backup ReplicaSet controller?
> A: No, but the controller-manager process is usually replicated (in HA setups) to prevent this issue.

Q: Can it recover state?
> A: Yes. Controllers are stateless; desired/actual state is stored in etcd, so they recover automatically.

``````
âœ… Corrected Understanding of HA Components in Kubernetes
---------------------------------------------------------
| Component                  | Stateless?     | HA Capable?     | Notes                                               |
|---------------------------|----------------|-----------------|-----------------------------------------------------|
| API Server                | âœ… Yes          | âœ… Yes          | Horizontally scalable; all replicas are active      |
| kube-controller-manager   | âŒ No *(Leader)*| âœ… Yes          | One active leader; others standby via leader election |
| kube-scheduler            | âŒ No *(Leader)*| âœ… Yes          | Same as controller-manager; leader-elected          |
| etcd                      | âŒ No           | âœ… Yes (Quorum) | Needs odd nodes (3/5); uses Raft consensus          |
| cloud-controller-manager  | âŒ No *(Leader)*| âœ… Yes          | Optional; used in cloud-native setups like GKE/EKS  |


## âœ… The Key Distinction:

> **"Controllers are stateless"** refers to **controller *logic***,
> while **"controller-manager is not stateless"** refers to **the running *process* needing leader election**.

---

### ğŸ” Breakdown:

### âœ… **Yes, controllers are logically stateless:**

* Controllers (like ReplicaSet, Deployment, etc.) **do not keep in-memory state** between restarts.
* All **desired state** (specs) and **actual state** (Pod statuses, events, etc.) are stored in **etcd**.
* So if the controller process (inside `kube-controller-manager`) crashes or is restarted:

  * Nothing is lost.
  * It **re-reads everything from etcd** and continues reconciling state.

ğŸ“Œ **This is why we say: â€œControllers are stateless.â€**

---

### â— But: **`kube-controller-manager` process is not stateless in HA setups**

* In a multi-replica HA setup, you **canâ€™t have all instances act at once**.
* Therefore, a **leader election mechanism** is needed to ensure **only one instance is active** at any given time.
* This doesn't mean the process holds permanent state â€” it just means:

  * **Only the leader executes logic** (like reconciling ReplicaSets)
  * Others **wait in standby**

ğŸ“Œ This is why we say: **"kube-controller-manager requires leader election"** â€” but **the logic it runs is stateless**.

---

## âœ… Final Summary (Interview-Ready):

> **Kubernetes controllers are logically stateless.**
> All state is stored in **etcd**, so if the controller restarts, it simply resumes by reading the desired and current state from etcd.
> However, in **HA setups**, only one instance of the controller-manager (or scheduler) is **active** at a time â€” this is enforced using **leader election**, not because of in-memory state, but to avoid conflicts in taking actions.

``````

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 4-ğŸ”  What is a reconciliation loop, and how frequently do controllers run it?

A reconciliation loop is a logic loop where a controller:
"Continuously compares the desired state (from etcd) with the actual state (from the cluster), and makes changes to match them."

âš™ï¸ Workflow:
-----------
- Controller watches a Kubernetes object (e.g., Deployment).
- Gets an event (add/update/delete).
- Fetches the current state (e.g., 1 Pod running, 3 desired).
- Takes action (e.g., create 2 more Pods).
- Waits for the next change/event.

â±ï¸ How frequently?
-------------------
- Based on events (watch API)
- Also runs periodic re-sync (every few minutes, e.g., 5 mins) to handle missed events.

â“ Follow-up Questions:
----------------------------
Q: Is reconciliation continuous?
> A: Yes, itâ€™s reactive (event-driven) + proactive (periodic).

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 5 - âš”ï¸How does the controller handle conflicts between actual and desired state?

The controller overwrites the actual state to match the desired state defined in etcd (via Deployment, etc.).

âš™ï¸ Workflow Example:
--------------------
- Desired state: replicas: 3
- Actual state: only 1 Pod running (maybe 2 were deleted manually)
- Controller detects mismatch
- It creates 2 new Pods to fix it

ğŸ›¡ï¸ Conflict Handling:
------------------------
- Controllers donâ€™t care why the actual state is different 
- They just enforce the desired state.
- Manual changes not reflected in desired spec will be reverted.
- Example: You manually delete a Pod â€” controller will recreate it.

â“ Follow-up Questions:
---------------------------
Q: What if someone manually changes a pod created by a ReplicaSet?
> A: The controller will revert or replace it.

# =========================================================================================================================================================
ğŸŒ©ï¸ Cloud Controller Manager (if on cloud like AWS/GCP)
-----------------------------------------------------------------------------------------------------------------------------------------------------------

# â˜ï¸ 1. What are the components of the Cloud Controller Manager?
CCM splits out cloud-specific logic from the main Kubernetes Controller Manager. It contains controllers that interact with the cloud providerâ€™s APIs.

ğŸ§± Key Components of CCM:
---------------------------
| Component              | Function                                              |
| ---------------------- | ----------------------------------------------------- |
| **Node Controller**    | Updates node status (e.g., shutdown, delete in cloud) |
| **Route Controller**   | Sets up network routes between nodes (VPC, etc.)      |
| **Service Controller** | Creates and manages LoadBalancers for Services        |
| **Volume Controller**  | Manages cloud storage (EBS, PD, etc.) for PVCs        |


ğŸ§  Behind the scenes:
------------------------
- Watches API objects like Node, Service, PersistentVolume
- Calls cloud provider APIs (e.g., AWS SDK) to provision infra

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”„ 2. How does CCM manage Load Balancers and Persistent Volumes?

âœ… Workflow for LoadBalancer Service:
---------------------------------------
- You create a Service with type: LoadBalancer.
- The Service Controller in CCM sees it.
- It calls the cloud provider API (e.g., AWS ELB or GCP LB).
- A load balancer is created and its IP is added to the Service status.

âœ… Workflow for PersistentVolume:
---------------------------------------
- You create a PVC.
- The Volume Controller:
    * Talks to the cloud provider (e.g., AWS EBS, GCP PD)
    * Provisions a disk
- A PersistentVolume is created and bound to the PVC.

â“ Follow-up:
---------------
Q: What is in-tree vs out-of-tree?
> A: CCM is part of the move to "out-of-tree" cloud providers (decouples cloud logic from Kubernetes core).

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸŒ 3. How does the CCM interact with cloud APIs?

 It uses cloud provider SDKs (e.g., AWS SDK, GCP SDK) to make API calls for:
- LoadBalancer creation/deletion
- Disk creation/attachment
- Route table setup
- Node lifecycle management

ğŸ“¦ Example:
```
cloudProvider.CreateLoadBalancer(service)
cloudProvider.AttachDisk(nodeName, volumeID)
```
â¡ï¸ This logic is implemented in cloud-specific CCM plugins (e.g., AWS CCM, GCP CCM).

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ’¥ 4. What happens if CCM fails or loses cloud API access?

âŒ Impacts:
- New LoadBalancers or Volumes will not be provisioned.
- Node status updates (e.g., instance shutdowns) may be missed.
- Routes may not be created.
- Existing Pods and Volumes still work (managed by kubelet/kube-proxy).

ğŸ” Recovery:
- When CCM restarts or API access is restored, it resumes reconciliation from etcd state.

â“ Follow-up:
Q: Will pods crash?
A: No, unless they depend on a resource that CCM failed to provision (like a PVC).

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”„ 5. How is CCM different from the regular Controller Manager?
| Aspect                  | Controller Manager                   | Cloud Controller Manager                        |
| ----------------------- | ------------------------------------ | ----------------------------------------------- |
| Purpose                 | Handles generic Kubernetes logic     | Handles **cloud-specific** infrastructure logic |
| Runs Controllers For    | Pods, ReplicaSets, Deployments, etc. | Nodes, LoadBalancers, Volumes, Routes           |
| Involved in Scheduling? | No                                   | No                                              |
| Talks to Cloud APIs?    | âŒ No                                 | âœ… Yes                                           |


âœ‚ï¸ Why Separate?
- Decouples cloud code from Kubernetes core.
- Enables out-of-tree providers (each cloud vendor can ship own CCM).


# =========================================================================================================================================================
ğŸ” Pod Lifecycle/Startup
-----------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ” 1. What is the complete lifecycle of a Pod from scheduling to termination?

Pending â†’ Scheduled â†’ Running â†’ Succeeded/Failed â†’ Terminating

ğŸ§¬ Workflow:
------------

- Pending: Pod created in API Server, but not yet scheduled to a node.
- Scheduled: Scheduler assigns it to a node.
- Running: kubelet pulls images, runs initContainers, then containers.
- Succeeded / Failed: Completed based on container exit codes.
- Terminating: 
   * kubectl delete pod or controller scale-down.
   * PreStop hooks and graceful shutdown handled.

ğŸ” Follow-up:
----------------
Q: Who handles Pod creation?
> A: kubelet on the target node fetches the Pod spec and manages container lifecycle via CRI.

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# âš™ï¸ 2. How do initContainers influence startup order?

`InitContainers` are **special containers that run *before*** your main application containers start in a Pod. They **run one after the other**, and **must all succeed** before the regular containers are started.
(mtlb saare init containers start hone chaiye before main container starts)


 ğŸ§  **Why do we need initContainers?**

You can think of them as **"setup steps"** for your application.

### âœ… **Use Cases:**

| Use Case                           | Example                                                        |
| -----------------------------------| -------------------------------------------------------------- |
| âœ… Wait for a database to be ready | Poll the DB until a connection is successful                   |
| âœ… Set permissions on a volume     | Run `chown` or `chmod` before the app starts                   |
| âœ… Download configs or secrets     | Pull a file from an external service before launching main app |
| âœ… Pre-warm a cache                | Populate Redis or local cache with required keys               |

---

### ğŸ”„ **Startup Workflow (Visual):**

```yaml
spec:
  initContainers:
    - name: init-db
      image: busybox
      command: ['sh', '-c', 'until nc -z db 5432; do sleep 1; done']
  containers:
    - name: app
      image: my-web-app
```

â¡ï¸ Step-by-step execution:

```
[init-db] --> (runs, exits successfully)
       â†“
[app]     --> (starts only after all initContainers are done)
```

---

### ğŸ” **Behavior Rules:**

* `initContainers` **always run first**, and in order (1 â†’ 2 â†’ 3â€¦).
* If one fails, **Kubelet will retry it** according to the Pod's `restartPolicy`.
* Once all initContainers succeed, they **do not run again**, even if app container crashes later.
* App containers will **not start** unless all initContainers complete successfully.

---

### â— Important Notes:

* `initContainers` can have their **own images, volumes, env vars, etc.**
* They **share the same network and volume** space as the main containers.
* Logs for `initContainers` are separate:

```bash
kubectl logs <pod> -c <init-container-name>
```

---

### âœ… Summary (Interview-friendly line):

> â€œ`initContainers` are like pre-steps in a Pod. They run sequentially before main containers and ensure preconditions (like DB readiness or volume setup) are met before the app starts. If any initContainer fails, the Pod wonâ€™t start until it succeeds.â€


-----------------------------------------------------------------------------------------------------------------------------------------------------------

# â¤ï¸â€ğŸ”¥ 3. What are the differences between liveness, readiness, and startup probes?

| Probe Type    | Purpose                                   | Effect when Fails                         |
| ------------- | ----------------------------------------- | ----------------------------------------- |
| **Liveness**  | Checks if container is **alive**          | Container is **restarted**                |
| **Readiness** | Checks if container is **ready**          | Pod is **removed from Service endpoints** |
| **Startup**   | Checks if container **finished starting** | Delays liveness & readiness checks        |

âœ… Liveness Probe â€“ How It Works (Step-by-Step)
------------------------------------------------

1. Probe worker starts: 
â†’ Kubelet reads the livenessProbe and starts a background checker for that container.

2. Wait before first check
â†’ It waits for initialDelaySeconds (e.g. 10s) to let the app start.

3. Probe runs every few seconds
â†’ It checks health using one of 3 ways:
* httpGet: Sends HTTP request (e.g. /status). 2xx/3xx = healthy.
* tcpSocket: Tries to connect to a port.
* exec: Runs a command inside the container (exit 0 = healthy).

4. If check fails: â†’ After failureThreshold (e.g. 3 fails), container is marked Unhealthy.

5. Container is restarted â†’ Kubelet stops it (SIGTERM then SIGKILL) and restarts it based on restartPolicy.

6. Cycle repeats after restart  â†’ Probe starts again from step 2.

ğŸ” Example with HTTP /status (httpGet):
```
livenessProbe:
  httpGet:
    path: /status
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5
  failureThreshold: 3
```
* If /status gives 200 â†’ healthy â†’ container keeps running
* If it gives 500 or no reply â†’ fails â†’ after 3 times â†’ container restarts

âœ… Other probe types:
* tcpSocket â†’ Just checks if port is open.
* exec â†’ Runs a script/command inside the container.

---

âœ… Readiness Probe â€“ How It Works (Step-by-Step)
-------------------------------------------------

1. Probe worker starts
â†’ Kubelet reads the readinessProbe and starts checking if the container is ready to receive traffic.

2. Wait before first check
â†’ It waits for initialDelaySeconds (e.g. 5s) before starting checks.

3. Probe runs repeatedly
â†’ It checks health using:
 * httpGet: Sends HTTP request to endpoint (e.g. /ready)
 * tcpSocket: Tries to connect to a port
 * exec: Runs a command inside the container

4. If check fails
â†’ Pod is removed from Service endpoints â€” it won't get traffic.

5. If check passes again
â†’ Pod is added back to the Service â€” starts receiving traffic again.

ğŸ” Example with HTTP /ready:
```
readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
  failureThreshold: 2
```
* If /ready gives 200 â†’ pod stays in load balancer
* If it gives 500 or fails â†’ pod is removed from load balancer (but not restarted)

âœ… Key Points:
* Does NOT restart the container â€” only controls traffic flow
* Used when app is running but not yet ready (e.g., DB not connected)

âœ… Other probe types:
* tcpSocket â†’ Checks if port is listening
* exec â†’ Runs a script or check inside the container

---

âœ… Startup Probe â€“ How It Works (Step-by-Step)
-------------------------------------------------

1. Probe starts immediately
â†’ Kubelet runs the startupProbe right after the container starts.

2. Runs until it succeeds or fails too many times
â†’ Only this probe runs during startup.

failureThreshold Ã— periodSeconds = max startup time

3. If startup probe passes
â†’ Kubelet enables liveness and readiness probes, and normal checks begin.

4. If it fails continuously
â†’ After failureThreshold, container is marked Unhealthy and restarted.

ğŸ” Example with HTTP /startup
```
startupProbe:
  httpGet:
    path: /startup
    port: 8080
  failureThreshold: 30     # allows up to 150s (30 Ã— 5)
  periodSeconds: 5

```
* App gets up to 150 seconds to start.
* If /startup gives 200 â†’ success â†’ normal liveness/readiness start.
* If it fails 30 times â†’ container is restarted.

âœ… Key Points:
* Used for slow-starting apps (e.g., Spring Boot)
* Prevents early liveness probe failures
* Disables other probes until it passes

âœ… Other probe types:
* httpGet, tcpSocket, exec all supported â€” same as other probes



ğŸ” Follow-up:
-------------
Q: What happens if readiness fails but liveness is OK?
A: Pod stays running, but won't receive traffic from Services.

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ§¨ 4. What happens when a probe fails repeatedly?

âœ… Liveness Probe fails repeatedly
-----------------------------------

* If it fails failureThreshold times in a row (default 3):
   * â†’ The container is marked Unhealthy
   * â†’ Kubelet restarts the container
   * â†’ After restart, the probe cycle starts again
   * â†’ If this keeps happening â†’ enters CrashLoopBackOff

âœ… Readiness Probe fails repeatedly
-------------------------------------

* The pod is removed from the Service endpoints
* â†’ It stays running, but wonâ€™t receive traffic
* â†’ Once the probe passes again â†’ itâ€™s added back

âœ… Startup Probe fails repeatedly
---------------------------------

* If it fails failureThreshold times
* â†’ Container is restarted
* â†’ Liveness and readiness probes never run if startup probe doesnâ€™t succeed

---

ğŸ” Probe failure triggers summary:
-------------------------------------

| Probe Type | Action on repeated failure                                  |
| ---------- | ----------------------------------------------------------- |
| Liveness   | âŒ Restart container                                         |
| Readiness  | ğŸš« Remove from Service (no restart)                         |
| Startup    | âŒ Restart container (disables other probes until it passes) |

-----------------------------------------------------------------------------------------------------------------------------------------------------------


# ğŸ“¦ 5. What is the order of container startup in a multi-container Pod?

#### âœ… **Startup Order:**

1. **`initContainers`** run **first**, one after another (sequentially).

   * Each must **complete successfully** before the next one starts.
2. After all `initContainers` finish:
   â†’ **All app containers** (`containers:` block) start **in parallel**.

---

### ğŸ” Example:

```yaml
initContainers:
  - name: init-db     # runs first
  - name: init-cache  # runs second, after init-db

containers:
  - name: web-app     # starts after all initContainers
  - name: sidecar     # starts at same time as web-app
```

---

### ğŸ§  Key Points:

* App containers **do not wait** for each other â€” they start together.
* **Startup order for main containers cannot be controlled**.
* If you need order between app containers â†’ use a **startup probe** or control logic inside the app.

---

**In short:**
`initContainers` run one-by-one first. Once done, all main containers start **together**. You cannot enforce a startup order between main containers.


# =========================================================================================================================================================

# âœ¨ Bonus â€“ Troubleshooting Components
-----------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸŸ  1. A pod is in Pending state for 10 minutes. How would you investigate?

âœ… Root Cause Possibilities:
-----------------------------

- No available nodes matching podâ€™s resource requests or constraints
- Unschedulable due to taints, affinity rules, or node selectors
- Image pull issues (if imagePullPolicy blocks scheduling)

ğŸ” Workflow to Debug:
---------------------
```
kubectl describe pod <pod-name>  # Look under "Events"
kubectl get events --sort-by='.lastTimestamp'
```

ğŸ” Check for:
---------------
- â€œ0/3 nodes availableâ€ â†’ Scheduler canâ€™t place Pod
- Insufficient memory/CPU â†’ Node doesn't meet resource requests
- Taints/Tolerations mismatch
- PVC not bound (storage issue)

â“ Follow-up:
--------------
"What if itâ€™s a DaemonSet pod?"
If a DaemonSet pod doesn't show up on a node, check these common blockers:

âœ… Things to check:
| Check                       | Why it matters                                                             |
| --------------------------- | -------------------------------------------------------------------------- |
| **NodeSelector**            | Does the Pod have a `nodeSelector` that excludes the current node?         |
| **Tolerations & Taints**    | Does the node have taints that the Pod can't tolerate?                     |
| **Host Ports**              | Is the host port already in use on the node? If yes â†’ pod can't bind to it |
| **Resource Requests**       | Does the Pod request more CPU/memory than available on the node?           |
| **Affinity/Anti-affinity**  | Any rules that prevent pod placement on that node?                         |
| **Pod Conditions / Events** | Use `kubectl describe pod <pod>` to see why itâ€™s pending or failed         |
| **Node Status**             | Is the node `Ready` and schedulable (`kubectl get nodes`)?                 |



-----------------------------------------------------------------------------------------------------------------------------------------------

# ğŸš« 2. Kubelet logs show container 'created' but not 'started' â€” what could be wrong?

âœ… Probable Issues:
--------------------
- Image pulled but container failed during startup
- Failing liveness/startup probe
- Permission issue (e.g., missing volume mount)
- Waiting for initContainers to finish

ğŸ§ª Workflow:
--------------
```
journalctl -u kubelet -f  # Live logs
kubectl describe pod <pod>
kubectl logs <pod-name> -c <container>
```
- Look for crash loops, probe failures
- Check container runtime logs (e.g., containerd)

â“ Follow-up:
---------------
"How does kubelet decide when to restart a container?" 
> Based on the pod's restartPolicy and probe failures

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# 3. API Server is unresponsive, but etcd is running â€” whatâ€™s your next step?

### âœ… **Answer (Short + Clear):**

> If the API Server is down but etcd is healthy, Iâ€™ll troubleshoot the API Server itself â€” checking its logs, static Pod health, and network access.

---

### ğŸ”„ **Troubleshooting Workflow:**

#### 1. âœ… **Check API Server Pod health**

```bash
kubectl get pods -n kube-system -o wide | grep apiserver
```

OR if kubectl is down (likely):

```bash
docker ps -a | grep kube-apiserver
# or
crictl ps | grep kube-apiserver
```

---

#### 2. ğŸ“„ **Inspect logs**

```bash
docker logs <apiserver-container-id>
# or
crictl logs <container-id>
# or static pod:
cat /var/log/pods/kube-system_kube-apiserver*/kube-apiserver/*.log
```

Look for:

* TLS/cert issues
* etcd connection errors
* port binding issues

---

#### 3. ğŸ§± **Verify static Pod manifest**

```bash
cat /etc/kubernetes/manifests/kube-apiserver.yaml
```

Check:

* Correct etcd endpoints
* Cert/key paths
* HostPort conflicts

---

#### 4. ğŸŒ **Check port 6443**

```bash
curl -k https://localhost:6443/healthz
```

If this fails, API server process may be running but unhealthy.

---

#### 5. ğŸ§  **System-level checks**

* `df -h`: Disk full?
* `top`: CPU/memory?
* `systemctl status kubelet`: Is kubelet healthy and managing the API server?

---

### âœ… Final line to say:

> Iâ€™ll check the API server logs and Pod status directly on the control plane node. Since etcd is fine, the issue is isolated to the API server itself â€” possibly config, port, or resource-related. My goal is to confirm whether itâ€™s running but unhealthy, or not starting at all.

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸŸ¥ 4. A node reports NotReady â€” what logs or components would you check?

âœ… Root Causes:
- kubelet is down or misconfigured
- Node network issue
- Docker/containerd failure
- High resource usage

ğŸ§ª Workflow:
- On the node:
```
systemctl status kubelet
journalctl -u kubelet -xe
```
- Check:   kubelet logs ,  container runtime status:
```
crictl info
systemctl status containerd
```
- Run:
```
kubectl describe node <node-name>
```
Check Conditions: block â€” it shows memory, disk pressure, PID pressure

â“ Follow-up:
â€œWhat if the kubelet is running but node is still NotReady?â€
â†’ Check if it can reach the control plane and DNS works

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ“ 5. Controller manager is reporting resource quota violations â€” how would you debug?
âœ… ResourceQuota limits the total CPU, memory, PVCs, etc., in a namespace.
ğŸ§ª Workflow:
```
kubectl describe resourcequota -n <namespace>
```
Look for usage vs hard limits

Then: ``` kubectl get pods -n <namespace> -o wide```
Identify which pods are consuming excess resources

âš ï¸ Example Scenario:
- Quota: 2 CPU, 4Gi memory
- Pod requests 3 CPU â†’ pod will stay Pending with quota violation

â“ Follow-up:
â€œWhat if quota is correct but pod still fails?â€
â†’ Might be a limitRange issue or request vs limit mismatch

-----------------------------------------------------------------------------------------------------------------------------------------------------------

# Deployment

âœ… Concept:
-------------

* Manages stateless apps.
* Ensures desired number of replica Pods are running.
* Supports rolling updates, rollbacks.
* Built on top of ReplicaSet.

ğŸ§  Key Features:
-----------------
* Declarative updates to Pods.
* Easy scaling via replicas:.
* Automatically replaces failed Pods.

ğŸ” Example:
----------
``````
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: nginx
        image: nginx
``````

---

### ğŸ” **How Deployment Rollback Works**

Kubernetes **stores a revision history** for every Deployment update. You can **revert** to a previous working version using:

```bash
kubectl rollout undo deployment <deployment-name>
```

---

### ğŸ§  **How it works internally:**

1. Each time you change the Deployment spec (e.g., new image, env, etc.), Kubernetes creates a **new revision**.
2. It saves the spec in the Deployment's **`.spec.revisionHistoryLimit`** (default: 10).
3. If something breaks, you can rollback to the previous version using the `undo` command.

---

### âœ… **Real Example**

#### Step 1: Create a deployment

```bash
kubectl create deployment myapp --image=nginx:1.20
```

#### Step 2: Update the image

```bash
kubectl set image deployment myapp nginx=nginx:1.25
```

#### Step 3: Check rollout status

```bash
kubectl rollout status deployment myapp
```

#### Step 4: Something goes wrong? Rollback!

```bash
kubectl rollout undo deployment myapp
```

â†’ This will revert to the previous image `nginx:1.20`.

#### Step 5: Check rollout history

```bash
kubectl rollout history deployment myapp
```

---

### ğŸ“ Notes:

* Rollback only works if **at least one previous revision** exists.
* You can also rollback to a specific revision:

```bash
kubectl rollout undo deployment myapp --to-revision=2
```

---

### âœ… Summary (interview-friendly):

> Kubernetes Deployments support rollback by keeping revision history. If a new rollout breaks, I can use `kubectl rollout undo` to revert to the last working version. This works because Deployment stores past specs and tracks changes over time.

â“ Interview Questions:
------------------------
1- How does Deployment manage updates and rollbacks?

2- What happens if a node crashes with a Deployment?

3- How does it differ from StatefulSet?

----------------------------------------------------------------

## ğŸ§© **1. What if a Deployment is scaled to 0 â€” can I still rollback?**

#### âœ… Short Answer:
Yes, you can rollback. Scaling to 0 removes all pods, but the **Deployment object**, **ReplicaSets**, and **revision history** are preserved.

#### ğŸ” What Actually Happens Internally:

* When you run: ```bash kubectl scale deployment myapp --replicas=0``` Kubernetes sets `spec.replicas = 0`.
This means:
* All pods are deleted
* The Deployment object still exists in the cluster
* Associated ReplicaSets are retained (with 0 replicas)

> â— Nothing is deleted â€” just paused

Now, when you run: ``` kubectl rollout undo deployment myapp```

* Kubernetes reverts the Deployment spec to the **last known revision** (e.g. image, env, etc.)
* It **updates the ReplicaSet** template
* But **replicas = 0**, so no pods are created unless you scale up again

âœ… So you'd usually follow up with: ```kubectl scale deployment myapp --replicas=3 ```

#### ğŸ§  Key Takeaway:

> Rollback works **independent of pod count**. If replicas = 0, Kubernetes wonâ€™t auto-spawn pods even after rollback â€” you must scale it manually.

---

## ğŸ§© **2. I updated the image, but pods didnâ€™t restart â€” why?**

#### âœ… Short Answer: 
Because Kubernetes saw **no actual change** in the Deployment template, so it didnâ€™t trigger a new rollout.

#### ğŸ” Deep Explanation: 
Letâ€™s say your Deployment YAML already has:
```yaml
image: myapp:1.0
```

Now you run:
```bash
kubectl set image deployment myapp myapp=myapp:1.0
```
Even though you â€œsetâ€ it again, itâ€™s the **same value**. Kubernetes compares:
* `spec.template` hash (annotations + pod spec)

If thereâ€™s **no difference**, it assumes:
> "Nothing to do â€” no new rollout needed."

#### ğŸ§ª Example:

```bash
kubectl set image deployment web nginx=nginx:1.21
kubectl set image deployment web nginx=nginx:1.21  # Again â†’ no effect
```

âœ… To **force a restart**, you must:

```bash
kubectl rollout restart deployment web
```

This triggers a **template hash change** by updating the `metadata.annotations`:

```yaml
kubectl.kubernetes.io/restartedAt: "2025-06-22T13:45:00Z"
```

Kubernetes treats this as a change â†’ rollout begins.


#### ğŸ§  Key Takeaway:

> If thereâ€™s no change in `spec.template`, Kubernetes wonâ€™t rollout.
> Use `rollout restart` to force restart even if image/config didnâ€™t change.

---

## **3. "You applied a faulty Deployment spec. How would you rollback if `kubectl` isn't working?"**

âœ… **Answer:**

answer explore : but using curl we can do that 

---

## **4. "When I change something in the Deployment (e.g., new image), who ensures the right pods are running?"**
âœ… **Answer:**

**The Deployment creates and manages the ReplicaSet**, but the **ReplicaSet is the one directly responsible** for:
* Keeping the desired number of pods running
* Re-creating pods if they crash or are deleted
So:
> âœ… **ReplicaSet is the object that directly manages the Pods.**
> ğŸ” Deployment manages the lifecycle of ReplicaSets.

ğŸ§  Example:

1. You run:
```bash
kubectl set image deployment myapp myapp=myapp:v2
```

2. Kubernetes does:
   * Creates a **new ReplicaSet** for v2
   * Scales down the **old ReplicaSet**
   * The new ReplicaSet launches new pods

3. If pods crash â†’ the **ReplicaSet** recreates them

âœ… Interview-Ready Answer:

> When I update a Deployment, it creates a new ReplicaSet version. The ReplicaSet is the object that directly manages the pods (ensuring the right number, restarting if needed). The Deployment itself manages rollout and rollback by controlling its ReplicaSets.

---

## **5. "What happens if I delete a Deployment manually â€” do pods stay?"**

âœ… **Answer:**

âœ… **Simplified Explanation:**

When you delete a Deployment in Kubernetes:

* It **automatically deletes** the ReplicaSet linked to it
* And the ReplicaSet then deletes the pods it created

So usually, the **pods also go away** â€” though you might still see them for a few seconds because Kubernetes shuts them down gracefully.

---

But if you delete the Deployment using a special flag like this:

```bash
kubectl delete deployment myapp --cascade=orphan
```

Then the Deployment is deleted, but the **ReplicaSet is left behind**.

In that case:

* The old ReplicaSet is still active
* It **can keep recreating pods** if they crash or are deleted

ğŸ§  Think of it like this:
> Delete Deployment â†’ normally deletes everything below it (ReplicaSet + Pods)
> Use `--orphan` â†’ only deletes the Deployment, leaves the rest running



## **6. "Why might a Deployment rollback silently fail?"**

âœ… **Answer:**

A rollback might silently fail if there's nothing to roll back to, or if the previous revision is identical to the current one. Kubernetes wonâ€™t roll out again unless it detects an actual change in the Deploymentâ€™s pod template.

âœ… **Example Scenario: Silent Rollback Failure**

You have a Deployment running this container:

```yaml
spec:
  template:
    spec:
      containers:
        - name: app
          image: myapp:v1
```

#### ğŸ” Step 1: You update the image to a new version

```kubectl set image deployment myapp app=myapp:v2```

âœ… Kubernetes:
* Creates a new ReplicaSet
* Rolls out new pods using `myapp:v2`

#### ğŸ”„ Step 2: You try to rollback
```kubectl rollout undo deployment myapp```
â¡ This works. Now you're back to `myapp:v1`.

#### ğŸ” Step 3: You **try to rollback again**
``` kubectl rollout undo deployment myapp```

âŒ Now nothing happens. Why? Whatâ€™s really going on:
Kubernetes compares the current pod template (with `myapp:v1`) to the previous revision â€” **which is also `myapp:v1`**. Since thereâ€™s **no change**, Kubernetes **does not trigger a rollout**.

> This is called a **no-op rollback** â€” rollback command ran, but the Deployment stayed the same.


#### ğŸ§  Another common case:

You apply a bad config once, then fix it manually (not via rollout), and now try to undo. But Kubernetes canâ€™t find any previous version to go back to â€” so rollback does nothing.
Also happens if: ```revisionHistoryLimit: 1```
Old revisions are **garbage collected**, and rollback fails silently.

#### âœ… Final Summary:
* A Deployment rollback may silently fail when Kubernetes sees no meaningful change to apply â€” usually because:
 * You're already on the same version
 * There's no previous revision
 * History was deleted

---

## **7. "Can you control how many Pods are updated at once?"**

âœ… **Answer:**
Yes, Kubernetes lets you control how many Pods are created or deleted during an update using maxSurge (extra pods allowed) and maxUnavailable (old pods that can go down) â€” both can be set as a number or percentage.

ğŸ” Deeper Explanation:
In a Deployment spec, you define the rollout strategy like this:
```
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 25%           # How many **extra** pods can be created
      maxUnavailable: 1       # How many **existing** pods can be taken down

```
ğŸ§  What these mean:
| Field            | Meaning                                                                     |
| ---------------- | --------------------------------------------------------------------------- |
| `maxSurge`       | Extra pods **added temporarily** during the update. Enables faster rollout. |
| `maxUnavailable` | Number of **old pods** that can be unavailable during the update.           |

ğŸ§ª Example:
Deployment has 4 replicas, and you update the image
```
maxSurge: 1
maxUnavailable: 1
```

Kubernetes will:
* Spin up 1 extra pod (5 pods total for a short time)
* Kill 1 old pod while new one comes up
* Keep doing this until rollout finishes

âœ… Summary for Interview:
Yes, I can control pod update concurrency using maxSurge and maxUnavailable. This allows me to tune rollout speed and availability during updates. Itâ€™s especially useful for large-scale or highly available apps.


---

## 8. "Does `kubectl delete pods` affect a Deployment?

No permanent effect â€” the Deploymentâ€™s **ReplicaSet recreates** the pods immediately. This is part of the self-healing behavior.
but agr delete deployement kra then deployement delete hoga wo replicaset ko delete krega and in turn pods ko 
and ya to replicaset ko 0 kro 

---

## **9. "Can I manually edit a ReplicaSet created by a Deployment?"**

âœ… **Answer:**
You can, but you shouldn't â€” because the Deployment controller will overwrite your changes.

#### ğŸ” Detailed Explanation:
* When you create a Deployment, Kubernetes automatically creates and manages a ReplicaSet underneath it.
* This ReplicaSet:
  * Is owned by the Deployment
  * Gets its spec (template, labels, etc.) directly from the Deployment
* If you try to manually edit the ReplicaSet, for example ```kubectl edit replicaset <name>```
* And change:
    * The pod image
    * The number of replicas
    * The pod labels or annotations
* Then: âŒ Your change will not persist.
* âœ… The Deployment controller will revert it back during its sync loop, because it expects full control over the ReplicaSet.

#### ğŸ§  Real-World Use Case:
* The only safe edits you can make are non-pod-affecting fields, like maybe metadata annotations, but even those might be overwritten.
* If you want to change how pods behave: âœ… Edit the Deployment, not the ReplicaSet.

#### âœ… Interview-Ready Answer:
No, I shouldnâ€™t manually edit a ReplicaSet created by a Deployment. The Deployment controller manages it and will override any manual changes. If I need to change pod behavior, I should edit the Deployment instead.

=================================================================================

# Stateful 

A StatefulSet is a Kubernetes controller used to manage stateful applications that need:
* Stable network IDs (like pod-0, pod-1, ...)
* Stable persistent storage per pod
* Ordered startup and shutdown (one at a time)

#### âœ… Key Features:
| Feature               | Description                                                                                |
| --------------------- | ------------------------------------------------------------------------------------------ |
| Stable Pod names      | Pods named as `app-0`, `app-1`, etc. (not random)                                          |
| Sticky storage        | Each pod gets its own PVC (`claim-0`, `claim-1`), which is **not deleted** when the pod is |
| Ordered rollout       | Pods start **and stop in order**, not in parallel                                          |
| Headless service req. | Uses `clusterIP: None` to allow direct DNS (`pod-0.service.namespace.svc`)                 |

#### ğŸ”§ Example Use Cases:
* Databases like Cassandra, MongoDB, PostgreSQL
* Zookeeper, Kafka brokers
* Applications needing fixed identity or persistent data

#### ğŸ¯ Twisted Interview Questions & Answers

â“1. "Can I scale a StatefulSet down from 3 to 1? What happens to PVCs of pod-2 and pod-1?"
* âœ… Answer: Yes, you can scale down. Pods app-2 and app-1 will be deleted in reverse order.
BUT their PVCs (claim-2, claim-1) are not deleted â€” they stay in the cluster.
This helps with future scaling or recovery.

---

â“ 2. Why does StatefulSet have a `replicas` field? OR kya statefulset me bhi replica set hota hai ?
* âœ… Answer: Even though StatefulSet gives **unique identity** to each pod, it still manages **how many pods** should exist â€” just like a Deployment or ReplicaSet.
So:
```yaml
spec:
  replicas: 3
```
Means Kubernetes will create:

* `pod-0`
* `pod-1`
* `pod-2`

But **unlike ReplicaSet**, these pods:

* Are created in **order**
* Have **stable names**
* Get **individual volumes**

> ğŸ” StatefulSet manages "N distinct pods", not "N identical pods" like ReplicaSet.

---

â“ 3. Can I delete `pod-0` manually in a StatefulSet?

âœ… **Yes,** you can: ```kubectl delete pod myapp-0```

Kubernetes will:
* Automatically recreate `myapp-0`
* Reattach its **existing PVC**

But âš ï¸ **donâ€™t delete the PVC** manually:

* PVC holds the podâ€™s **data**
* Deleting it means the new pod-0 **starts empty**

ğŸ§  Bonus Tip:

To simulate a crash: ```kubectl delete pod myapp-0 --grace-period=0 --force```. It will still come back with the **same name and data**, as long as the PVC is intact.

âœ… Interview-ready line:
> StatefulSet uses `replicas` to define how many uniquely named pods should run. You can delete individual pods like `pod-0`, and Kubernetes will recreate them with the same identity and volume â€” preserving their state.

---

â“4. "Why do StatefulSets need a headless service?"
*  âœ… Answer:
> **StatefulSets need a headless service** (`clusterIP: None`) to assign **stable DNS names** to each pod like:
```
pod-0.myservice.default.svc.cluster.local
```
This allows:
* **Pod-to-pod direct communication**
* **Apps like databases** (e.g., Kafka, MongoDB) to refer to specific peers

Without a headless service:
* Kubernetes load-balances between pods randomly
* You **canâ€™t target individual pods by name**

 ğŸ§  Example:
* Kafka broker `broker-1` wants to talk to `broker-0`, it uses:
```bash
broker-0.kafka.default.svc.cluster.local
```
* Thatâ€™s only possible because of the **headless service** exposing named pods.

âœ… One-liner Summary:  Headless service gives each StatefulSet pod a unique, stable DNS â€” critical for apps that need fixed identity and peer awareness.

---



```
NOTE:  HEADLESS SERVICE means "cluster IP : none" .

### ğŸ’¡ **What is a Headless Service?**
> A **headless service** is just a Kubernetes Service with: ```clusterIP: None```
This tells Kubernetes:
âŒ Donâ€™t assign a virtual IP
âœ… Just publish **DNS records of all backend pods**

 ğŸ¤ In context of controllers:
-----------------------------------------

| Controller      | Headless Service | Required?    | Behavior                                                                   |
| --------------- | ---------------- | ------------ | -------------------------------------------------------------------------- |
| **StatefulSet** | âœ… Yes (required) | âœ… Required   | Needed for **stable DNS per pod** like `pod-0.svc`                         |
| **Deployment**  | âœ… Optional       | âŒ Not needed | Default service gives 1 IP (load-balanced), headless gives **all pod IPs** |


### ğŸ§ª `dig` / `nslookup` Behavior:
----------------------------------------

A. â–¶ï¸ Normal Service (with clusterIP): ```dig myapp.default.svc.cluster.local```
â¡ Returns **1 IP only** (load balancer / kube-proxy does round-robin behind it)

B. â–¶ï¸ Headless Service (with clusterIP: None): ```dig myapp-headless.default.svc.cluster.local```
â¡ Returns **all backend pod IPs**

 âœ… Final Summary (One-liner):
--------------------------------
> `clusterIP: None` makes a Service headless. Itâ€™s **required in StatefulSet** to give each pod a DNS name. In Deployments, itâ€™s **optional** and only needed if you want to **access all pod IPs individually** (no load balancing).
```

---

â“5. "How does rolling update work in StatefulSets?"
* âœ… Answer: Pods are updated one at a time, in order (pod-0, then pod-1, ...). Each pod must be ready before the next one updates. Ensures consistency and safety for stateful workloads.

---

â“6. "Can StatefulSet guarantee data recovery after a crash?"
* âœ… Answer: Only if PVCs are backed by durable storage (e.g., EBS, GCE PD).
Kubernetes wonâ€™t delete the PVCs, but data durability is storage-dependent.

=================================================================================

#  PV & PVC 

âœ… What is a PersistentVolume (PV)?
-------------------------------------
* A PV is a **cluster-wide** storage resource thatâ€™s provisioned by an admin(static) or dynamic storage class.
* It represents actual storage (like EBS, NFS, GCE disk) ```kind: PersistentVolume```

âœ… What is a PersistentVolumeClaim (PVC)?
-----------------------------------------
* A PVC is a user request for storage.
* It says: â€œI want 10Gi of storage with ReadWriteOnce access.â€
``` 
kind: PersistentVolumeClaim
```
* PVC is matched (bound) to a suitable PV behind the scenes.

âœ… Real-life Analogy (technical):
-----------------------------------
* PV = physical hard disk available in cluster
* PVC = request to use a portion of it
* Pod = mounts PVC â†’ accesses storage

âœ… Workflow
--------------
``` Pod â†’ PVC â†’ StorageClass â†’ CSI Driver â†’ PV â†’ EBS ```

#### 1. Pod declares a volume using a PVC
```
volumes:
  - name: my-vol
    persistentVolumeClaim:
      claimName: mypvc
```
* Pod just says: â€œI want the PVC named mypvc.â€

#### 2. PVC is a storage request
```
kind: PersistentVolumeClaim
spec:
  storageClassName: gp3      # uses EBS CSI driver
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 5G
```
* PVC says: â€œI need 5Gi storage, provisioned via gp3 StorageClass.â€

#### 3. StorageClass â†’ triggers CSI driver
```
kind: StorageClass
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
```
* âœ… The ebs.csi.aws.com driver is called â†’ it creates an EBS volume

#### 4. CSI Driver creates actual storage (EBS)
* Behind the scenes:
  * AWS EBS volume is created (e.g., vol-0a123...)
  * Kubernetes auto-creates a PersistentVolume (PV) for it
  * The PV looks like:
```
kind: PersistentVolume
spec:
  capacity:
    storage: 5Gi
  csi:
    driver: ebs.csi.aws.com
    volumeHandle: vol-0a123456
    fsType: ext4
```
#### 5. PV binds to the PVC
* Kubernetes links the auto-created PV to the pending PVC
* â†’ PVC status = Bound

#### 6. Pod mounts the volume
```
volumeMounts:
  - name: my-vol
    mountPath: /data
```
* Pod now sees EBS volume mounted at /data, and data is persistent across pod restarts or node failures.

âœ… Final Visual (Cloud Dynamic Case)
```
Pod
 â””â”€> PVC
      â””â”€> StorageClass (gp3)
            â””â”€> CSI Driver (ebs.csi.aws.com)
                  â””â”€> EBS volume (AWS)
                        â””â”€> PV (auto-created)

```

---

## Interview Questions on PV & PVC 

â“1. "What happens when a pod using a PVC is deleted?"
âœ… Answer:
- The PVC stays (unless explicitly deleted).
- The PV stays, but whether itâ€™s reused or deleted depends on reclaim policy:
   * Retain: PV stays, must be cleaned manually
   * Delete: PV & underlying storage auto-deleted

---

â“2. "Can multiple pods use the same PVC?"
âœ… Answer:
* Depends on access mode:
   * ReadWriteOnce â€“ one pod at a time (most common, like AWS EBS)
   * ReadOnlyMany â€“ many pods, read-only
   * ReadWriteMany â€“ many pods, read/write (e.g., NFS, GlusterFS)

---

â“3. "What happens if thereâ€™s no matching PV for a PVC?"
âœ… Answer:
* PVC stays in Pending state
* Scheduler waits until a matching PV is available
* Or, if using StorageClass, one can be provisioned dynamically

---

â“4. "Can a PV be reused after being released?"
âœ… Answer:
* Only if reclaimPolicy: Retain and **the PV was statically provisioned**, the admin must manually:
   * Remove old claim info (like claimRef)
   * Set status back to Available
   * Rebind to a new PVC

ğŸ” **What about dynamically provisioned PVs?**
* If a PVC was dynamically created using a StorageClass:
   * Most StorageClasses set reclaimPolicy: Delete**
   * So when PVC is deleted, the PV and backing disk (e.g., EBS) are also deleted
   * No reuse is possible â€” new PVC = new PV + new disk

---

â“5. Is EBS CSI driver a PV?
âœ… Answer:
The EBS CSI driver is not a PV, but a plugin that enables Kubernetes to dynamically create and manage EBS-backed PVs using PVCs. Itâ€™s part of the CSI standard and replaces the older in-tree AWS volume plugin.

---

â“6. "Does deleting a PVC delete data?"
âœ… Answer:
* Only if the PVâ€™s reclaimPolicy is set to Delete.
* If itâ€™s Retain, data stays â€” even if PVC is deleted.

========================================================================================

# STORAGE CLASS

#### ğŸ“¦ What is a **StorageClass** in Kubernetes?
> A **StorageClass** defines how **dynamic storage** should be **provisioned automatically** in Kubernetes.
> It tells Kubernetes:

* **Which storage plugin (provisioner)** to use
* **What type of disk** (e.g. gp3, standard, SSD)
* **How to create it** (parameters like fsType, encryption)

#### âœ… Why it's needed?

Without a StorageClass:

* Admin must create PersistentVolumes (PVs) **manually**
* No automation â€” error-prone and not scalable

With StorageClass:

* When you create a PVC, it **auto-creates a matching PV**
* Backed by cloud volumes like **EBS (AWS)**, **PD (GCP)**, **Disk (Azure)**, **NFS**, etc.


#### ğŸ§  Key Fields in StorageClass
| Field               | Purpose                                               |
| ------------------- | ----------------------------------------------------- |
| `provisioner`       | CSI driver to call (e.g., `ebs.csi.aws.com`)          |
| `parameters`        | Extra options (e.g., `type: gp3`, `fsType: ext4`)     |
| `reclaimPolicy`     | What to do when PVC is deleted (`Delete` or `Retain`) |
| `volumeBindingMode` | When to bind (e.g., `WaitForFirstConsumer`)           |


#### ğŸ“„ Sample StorageClass YAML (for AWS EBS)

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ebs-gp3
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  fsType: ext4
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
```

#### ğŸ¯ Twisted Interview Questions

â“1. Can I have multiple StorageClasses?
> âœ… Yes. Each can provision a different type of disk (e.g., SSD, HDD, encrypted).

â“2. What if I donâ€™t specify `storageClassName` in PVC?
> If a **default StorageClass** exists, it is used automatically.

â“3. What is `volumeBindingMode: WaitForFirstConsumer`?
> It delays volume creation **until a pod is scheduled**, so that Kubernetes knows which **zone** to provision the disk in (important for **zonal storage like EBS**).

#### âœ… Final Interview Summary:
> A **StorageClass** automates the creation of PersistentVolumes by defining **how to dynamically provision storage** using CSI drivers. It's like a storage blueprint for your PVCs.

=====================================================================================

# SERVICE ACCOUNT

#### ğŸ§‘â€ğŸ’» What is a **Service Account (SA)** in Kubernetes?
> A **ServiceAccount** is an identity used by **Pods** to interact securely with the **Kubernetes API server** or external services.

#### âœ… Why is it used?
* To **authenticate** Pods to the API Server
* To provide **fine-grained permissions** using RBAC (e.g., can read Secrets, can't delete Pods)
* Automatically **injects tokens** into running pods (`/var/run/secrets/kubernetes.io/serviceaccount/`)

#### ğŸ§  Key Points
| Feature       | Detail                                                                 |
| ------------- | ---------------------------------------------------------------------- |
| Default SA    | Each namespace has a built-in `default` ServiceAccount                 |
| Token Mount   | Pods get a JWT token automatically mounted inside                      |
| Permissions   | Defined using `Role` or `ClusterRole` + `RoleBinding`                  |
| Custom SA     | You can create your own and assign to specific Pods                    |
| Secure access | Used for access to secrets, configmaps, or calling API Server securely |

#### ğŸ“„ Example Workflow

### 1. Create a ServiceAccount

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: reader
```

---

### 2. Bind a Role to it

```yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-secrets
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "list"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: bind-reader
subjects:
- kind: ServiceAccount
  name: reader
roleRef:
  kind: Role
  name: read-secrets
  apiGroup: rbac.authorization.k8s.io
```

---

### 3. Attach it to a Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  serviceAccountName: reader   # ğŸ‘ˆ assign SA to Pod
  containers:
  - name: app
    image: my-image
```

```
â“ Why canâ€™t we attach a Role directly to a Pod?
* Because in Kubernetes: 
  * ğŸ” RBAC is designed to work with identities â€” not resources.
  * and Pods are not identities â€” ServiceAccounts are.

ğŸ” Core Reason:
* Roles and ClusterRoles define what an identity (user or ServiceAccount) can do.
* Pods don't have identity themselves.
* So you can't say: "This Pod can read secrets."
* Instead, you say: "This ServiceAccount (used by the Pod) can read secrets."

ğŸ”§ Bonus: Why not attach Role to Pod directly?
* Pods are ephemeral â€” they can be deleted/restarted any time.
* RBAC needs a stable, reusable identity â€” ServiceAccount provides that.
* Reusing a RoleBinding for many Pods becomes clean and maintainable.

```

---

## ğŸ¯ EXAMPLE

### ğŸ” Use Case:
> You have a **backend service** running inside a Pod that needs to:
  * Access a **Secret** in the cluster (e.g., DB credentials)
  * Read it directly using the **Kubernetes API** (not via mounted volume)

#### ğŸ” What Happens:

##### ğŸ”¸ **1. App makes an HTTP call to the API Server**:
```http
GET /api/v1/namespaces/default/secrets/db-secret
Authorization: Bearer <token>
```

##### ğŸ”¸ **2. Where does the token come from?**
* Kubernetes automatically mounts a JWT **ServiceAccount token** inside the Pod at:

```
/var/run/secrets/kubernetes.io/serviceaccount/token
```

* Your app reads this token and includes it in the Authorization header.

##### ğŸ”¸ **3. If you're using the default ServiceAccount...**
* âŒ Access is **denied** (403 Forbidden)
* Because **default SA has no RBAC permissions**.

##### ğŸ”¸ **4. You create a custom ServiceAccount with a Role:**
* You define a `read-secrets` role (with access to Secrets)
* You bind it to a new ServiceAccount: `backend-reader`
* You assign this SA to your Pod using:

```yaml
serviceAccountName: backend-reader
```

Now your appâ€™s in-cluster API request **succeeds**, because the token it uses is tied to a ServiceAccount **with proper permissions**.

## ğŸ§ª Real-World Examples

| Use Case                                | Why ServiceAccount is Needed                          |
| --------------------------------------- | ----------------------------------------------------- |
| Jenkins Pod accessing K8s to deploy     | Jenkins needs SA token + Role to call `kubectl apply` |
| ArgoCD syncing apps                     | ArgoCD uses SA to interact with K8s manifests         |
| Fluentd reading logs                    | Needs access to Pod logs or container metadata        |
| Custom app fetching Secrets at runtime  | Needs RBAC + SA token to query K8s Secrets API        |
| K8s Job creating ConfigMaps dynamically | Needs Role to create/update ConfigMaps                |

#### âœ… Final Takeaway (One-liner):
> **ServiceAccounts are how Pods prove their identity inside the cluster.** With RBAC, they define what that Pod can do â€” like accessing Secrets, listing Pods, or applying configs.


#### ğŸ¯ Common Interview Questions

â“1. Whatâ€™s the difference between a user and a service account?
> âœ… User = human identity (external), ServiceAccount = used by pods

â“2. Can a ServiceAccount access all API resources by default?
> âœ… No. It has **no privileges** unless you bind a Role or ClusterRole

â“3. What happens if I donâ€™t specify `serviceAccountName` in a pod?
> âœ… Kubernetes assigns the **default SA** of the namespace

#### âœ… Final Summary (Interview Line):
> A ServiceAccount provides **Pod-level identity** in Kubernetes, used to access the API securely. It works with **RBAC** for fine-grained permissions and is mounted as a token inside the container.

========================================================================

# ROLE , ROLE BINDING , CLUSTER ROLE , CLUSTER ROLE BINDING 

All four are used in **RBAC (Role-Based Access Control)** in Kubernetes to manage **who can do what**.

---

## ğŸ§± 1. **Role**

> Defines **permissions** within a **specific namespace**

```yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: pod-reader
  namespace: dev
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]
```

âœ… Use when: You want to **limit access to one namespace** only.

---

## ğŸ”— 2. **RoleBinding**

> Binds a **Role** to a **user, group, or ServiceAccount** â€” **in that same namespace**

```yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: bind-pod-reader
  namespace: dev
subjects:
- kind: ServiceAccount
  name: backend-app
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```

âœ… Use when: You want to apply the Role to a subject **in one namespace**.

---

## ğŸŒ 3. **ClusterRole**

> Like `Role`, but:

* Itâ€™s **cluster-wide**
* Can grant access to **non-namespaced resources** (e.g., nodes, persistent volumes)
* Can also be reused across many namespaces

```yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-nodes
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list"]
```

âœ… Use when: You need access across **all namespaces** or to **cluster-level resources**.

---

## ğŸŒ 4. **ClusterRoleBinding**

> Binds a `ClusterRole` to a subject across the **whole cluster**

```yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: bind-read-nodes
subjects:
- kind: ServiceAccount
  name: backend-app
  namespace: dev
roleRef:
  kind: ClusterRole
  name: read-nodes
  apiGroup: rbac.authorization.k8s.io
```

âœ… Use when: You want to give **cluster-level access** to a user/SA.

---

## ğŸ§  Quick Comparison Table

| Type                 | Scope        | Can Access Cluster Resources? | Used For                                   |
| -------------------- | ------------ | ----------------------------- | ------------------------------------------ |
| `Role`               | Namespace    | âŒ No                          | Namespaced access only                     |
| `RoleBinding`        | Namespace    | âŒ No                          | Bind Role to SA/user in a namespace        |
| `ClusterRole`        | Cluster-wide | âœ… Yes                         | Access cluster resources / all namespaces  |
| `ClusterRoleBinding` | Cluster-wide | âœ… Yes                         | Bind ClusterRole to subject (cluster-wide) |

---

## âœ… Final Interview Summary

> **Roles** define what actions are allowed.
> **Bindings** assign those roles to identities.
> Use **Role + RoleBinding** for namespace access, and **ClusterRole + ClusterRoleBinding** for cluster-wide or non-namespaced resource access.

## EXAMPLE 

### âœ… 1. **Role + RoleBinding**
#### ğŸ¯ Use Case:

A dev team working in the `dev` namespace should **only view Pods and Secrets** in that namespace.

#### ğŸ”§ Implementation:

* âœ… Create a **Role** to allow `get`, `list` on Pods and Secrets.
* âœ… Bind that Role to their **ServiceAccount** using a **RoleBinding**.

```yaml
# Role (namespaced)
kind: Role
metadata:
  name: dev-reader
  namespace: dev
rules:
- apiGroups: [""]
  resources: ["pods", "secrets"]
  verbs: ["get", "list"]
```

```yaml
# RoleBinding
kind: RoleBinding
metadata:
  name: bind-dev-reader
  namespace: dev
subjects:
- kind: ServiceAccount
  name: app-reader
roleRef:
  kind: Role
  name: dev-reader
```

> ğŸ”’ Access is limited to **dev** namespace only.

---

### âœ… 2. **ClusterRole + ClusterRoleBinding**

#### ğŸ¯ Use Case:

Youâ€™re running **Prometheus** or a **monitoring agent** that needs to **read all Node and Pod metrics** across the cluster.

#### ğŸ”§ Implementation:

* âœ… Create a **ClusterRole** to allow access to `nodes`, `pods`, etc.
* âœ… Bind that ClusterRole to the ServiceAccount of Prometheus using **ClusterRoleBinding**.

```yaml
# ClusterRole
kind: ClusterRole
metadata:
  name: prometheus-reader
rules:
- apiGroups: [""]
  resources: ["nodes", "pods"]
  verbs: ["get", "list"]
```

```yaml
# ClusterRoleBinding
kind: ClusterRoleBinding
metadata:
  name: bind-prometheus-reader
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: monitoring
roleRef:
  kind: ClusterRole
  name: prometheus-reader
```

> ğŸ”“ Access applies to **all namespaces + cluster resources** like nodes.

---

### âœ… Hybrid Use Case: Reuse `ClusterRole` in namespace

You can bind a **ClusterRole** to a **RoleBinding** for use **in one namespace**.

#### ğŸ¯ Example:

You want the same "pod-reader" access in every namespace.

* Instead of creating 10 Roles (one per namespace)â€¦
* You create **one ClusterRole**
* And use **RoleBindings** in each namespace

---

#### ğŸ¯ Final Takeaway (Interview line):

> Use `Role`/`RoleBinding` for namespaced use cases (e.g., dev apps accessing secrets), and `ClusterRole`/`ClusterRoleBinding` for cluster-wide tools (like Prometheus, cert-manager) or to access non-namespaced resources like nodes or volumes.

=====================================================================================

# INGRES & INGRES CONTROLLER


## ğŸŒ What is an **Ingress** in Kubernetes?

> An **Ingress** is an API object that defines **HTTP/HTTPS routing rules** to expose services to external users.

It lets you:

* Route traffic by **hostnames** or **paths**
* Use **TLS/SSL termination**
* Avoid exposing each service via NodePort or LoadBalancer

```
Q- How Ingress avoids exposing each service via NodePort or LoadBalancer?

## âš ï¸ Without Ingress:

Every external-facing service needs **its own NodePort or LoadBalancer**.

### Example:

| Service        | Type         | External Access                            |
| -------------- | ------------ | ------------------------------------------ |
| `frontend-svc` | LoadBalancer | [http://LB-IP-1](http://LB-IP-1)           |
| `api-svc`      | LoadBalancer | [http://LB-IP-2](http://LB-IP-2)           |
| `auth-svc`     | NodePort     | [http://NodeIP:32001](http://NodeIP:32001) |

ğŸ§¨ Result:

* **Wasted public IPs**
* **Hard to manage**
* **No hostname/path-based routing**

---

## âœ… With Ingress:

You expose **just ONE** external endpoint via the **Ingress Controller**, then define **routes inside Ingress rules**.

### Example:

```yaml
rules:
- host: myapp.com
  http:
    paths:
    - path: /frontend
      backend:
        service:
          name: frontend-svc
          port:
            number: 80
    - path: /api
      backend:
        service:
          name: api-svc
          port:
            number: 80


ğŸ” Now, all traffic goes like this:

http://myapp.com/frontend â†’ frontend-svc
http://myapp.com/api     â†’ api-svc


âœ… Only **one LoadBalancer** (for the Ingress Controller)

## ğŸ§  Summary (Interview-ready line):

> Ingress avoids the need to expose each service with its own LoadBalancer or NodePort by **consolidating traffic through a single entry point**, using **path- or host-based routing** handled by the Ingress Controller.
```

---


## ğŸš¦ What is an **Ingress Controller**?

> It is the **actual software (pod)** that watches Ingress objects and configures **reverse proxy rules** accordingly.
* ğŸ§© Ingress = rules
* ğŸ§© Ingress Controller = enforcement engine

**Popular Ingress Controllers:**
* `nginx-ingress-controller`
* `traefik`
* `HAProxy`
* `AWS ALB Ingress Controller`

---

## ğŸ” Workflow

```
Client â†’ DNS â†’ Ingress Controller (Nginx) â†’ Routes â†’ Service â†’ Pod
```

1. You deploy an Ingress Controller (e.g., NGINX) as a Pod
2. You write Ingress rules (like routing `app1.example.com â†’ app1-svc`)
3. Controller picks up rules and applies them to its internal proxy
4. External traffic comes to LoadBalancer IP â†’ hits Ingress Controller â†’ routed to correct service

---

## ğŸ” TLS Support

Ingress supports:

* HTTPS termination via `tls` block
* Store certificate in a `Secret`
* Automatically integrate with cert-manager for Letâ€™s Encrypt

---

## ğŸ§  Common Interview Questions

### â“1. What is the difference between Ingress and Ingress Controller?
> Ingress defines **rules**, Ingress Controller implements those rules.

---

### â“2. Can you expose TCP/UDP using Ingress?
> No. Ingress is for **HTTP/HTTPS**. For TCP/UDP, use a `LoadBalancer` or `NodePort`.

---

### â“3. How is Ingress different from LoadBalancer service?
> `LoadBalancer` exposes one service directly.
> `Ingress` can **consolidate many services** behind one external IP and route by path or host.

---

### â“4. Do you need both Ingress and Ingress Controller?
> âœ… Yes. Ingress alone does nothing without a controller.

---

## âœ… Final Summary

> **Ingress** is the way to expose multiple Kubernetes services over HTTP(S) using clean host/path-based routing.
> You must deploy an **Ingress Controller** (like NGINX) to actually process and forward traffic.

Q. What happens if there is only an Ingress object and no Ingress Controller
* The Ingress rules are not processed â€” no routing, no external access.
* Kubernetes just stores the Ingress resource, but:
  * No IP is assigned
  * No traffic is forwarded
  * No error shown, but it silently doesn't work
  * Ingress Controller is mandatory to make Ingress functional.

Q. Is ingres controller part of controller manager ?
* No , The Controller Manager handles native K8s objects.
* The Ingress Controller is a separate pod you must deploy to handle Ingress logic.

==============================================================================

# CONFIG MAP & SECRETS

> Use `ConfigMap` for non-sensitive app settings, and `Secret` for anything confidential like passwords, tokens, or API keys.
> Both can be **mounted as env vars or files**, and help decouple app logic from configuration.


#### ğŸ“¦ What is a `ConfigMap`?
* A `ConfigMap` is used to **store configuration data** (non-sensitive) as **key-value pairs**.

### âœ… Use case:

Pass environment variables, command-line args, or config files to containers.

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_MODE: "production"
  LOG_LEVEL: "debug"
```

### ğŸ”§ Mount example:

```yaml
env:
- name: APP_MODE
  valueFrom:
    configMapKeyRef:
      name: app-config
      key: APP_MODE
```

---

## ğŸ” What is a `Secret`?

A `Secret` stores **sensitive data** like passwords, tokens, or keys â€” **base64-encoded**.

### âœ… Use case:

Pass credentials to containers **securely**.

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
data:
  DB_USER: YWRtaW4=      # "admin"
  DB_PASS: cGFzc3dvcmQ=  # "password"
```

### ğŸ”§ Mount example:

```yaml
env:
- name: DB_USER
  valueFrom:
    secretKeyRef:
      name: db-secret
      key: DB_USER
```

## ğŸ” Goal: Use AWS Secrets Manager instead of Kubernetes Secret â€” and inject those secrets into the Pod securely.

#### âœ… Real-Life Flow Using External Secrets Operator (ESO)
```
Pod â†’ ServiceAccount â†’ IAM Role â†’ AWS Secrets Manager â†’ ESO â†’ Kubernetes Secret â†’ Mounted in Pod
```

#### ğŸ” Step-by-step Breakdown:

##### 1. Create secret in AWS Secrets Manager
Example:
```
{
  "DB_USER": "admin",
  "DB_PASS": "s3cr3t"
}
```

##### 2. Create IAM Role with access to that secret
* Allow secretsmanager:GetSecretValue
* Trust relationship: bound to your EKS ServiceAccount (IRSA)
```
{
  "Effect": "Allow",
  "Action": "secretsmanager:GetSecretValue",
  "Resource": "arn:aws:secretsmanager:region:acct-id:secret:my-db-secret"
}
```

##### 3. Create a Kubernetes ServiceAccount
```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: app-sa
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::<acct>:role/SecretsReaderRole
```

##### 4. Install External Secrets Operator (ESO) in the cluster
* It will:
  * Run a controller Pod
  * Periodically read from AWS Secrets Manager
  * Create/update corresponding Kubernetes Secret

##### 5. Define an ExternalSecret
```
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: db-creds
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: aws-secrets-store
    kind: SecretStore
  target:
    name: db-secret                 # Kubernetes Secret created
  data:
  - secretKey: DB_USER              # Key in Kubernetes Secret
    remoteRef:
      key: my-db-secret             # AWS Secrets Manager name
      property: DB_USER
  - secretKey: DB_PASS
    remoteRef:
      key: my-db-secret
      property: DB_PASS
```

##### 6. Mount the generated Kubernetes Secret into your Pod
```
env:
- name: DB_USER
  valueFrom:
    secretKeyRef:
      name: db-secret
      key: DB_USER
---
```

#### ğŸ§  Final Summary (Interview-ready):
> Yes, in production we use a Kubernetes ServiceAccount bound to an IAM Role (IRSA), which allows the External Secrets Operator to read AWS Secrets Manager values. Those are converted into Kubernetes Secrets and then mounted into Pods â€” ensuring secrets never need to be hardcoded or managed manually in Kubernetes.

================================================================================

# DAEMON SETS

## ğŸ§± What is a **DaemonSet**?

> A **DaemonSet** ensures **exactly one Pod runs on every node** (or some selected nodes) in the cluster.

### ğŸ” If a new node is added:
â†’ A DaemonSet Pod is automatically scheduled on it.

### ğŸ§½ If a node is removed:
â†’ The Pod is automatically cleaned up.

---

## âœ… Real-World Use Cases:

| Use Case                 | What the DaemonSet Pod does                         |
| ------------------------ | --------------------------------------------------- |
| ğŸ§ª **Monitoring Agents** | e.g., Prometheus Node Exporter, Datadog agent, etc. |
| ğŸ“¦ **Log Collectors**    | e.g., Fluentd, Filebeat, Logstash                   |
| ğŸ” **Security Daemons**  | e.g., Falco or Trivy to scan workloads on each node |
| ğŸ§° **Storage plugins**   | e.g., CSI drivers that need node-level components   |
| ğŸ§ª **Custom node tools** | Run system checkers or cleanup tasks node-by-node   |

---

## ğŸ§¾ Example DaemonSet YAML:

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: log-agent
spec:
  selector:
    matchLabels:
      app: log-agent
  template:
    metadata:
      labels:
        app: log-agent
    spec:
      containers:
      - name: fluentd
        image: fluent/fluentd
```

---

## ğŸ§  Interview Questions

### â“1. What happens when a new node is added?

> A DaemonSet Pod is automatically created on it.

### â“2. How is DaemonSet different from Deployment?

> Deployment: replicas across the cluster
> DaemonSet: **1 Pod per node**, not tied to replicas

### â“3. Can you run DaemonSet only on some nodes?

> âœ… Yes, use:

* `nodeSelector`
* `affinity`
* `tolerations` (for tainted nodes)

---

## âœ… Summary (Interview line):

> DaemonSets are used to **run 1 pod per node**, ideal for node-level agents like log collectors or monitoring tools. They automatically handle node joins/leaves and are essential for system-wide observability and security.

======================================================================================

# JOBS 


#### âš™ï¸ What is a **Job** in Kubernetes?
> A **Job** is used to run a **one-time task or batch process** â€” not a long-running service.

Unlike Deployments or DaemonSets, Jobs:
* Run to **completion**
* Ensure the task **finishes successfully a specific number of times**
* Retry on failure based on `backoffLimit`

---

## âœ… Real-World Use Cases

| Use Case                  | Description                                  |
| ------------------------- | -------------------------------------------- |
| ğŸ§¹ DB Migration           | Apply DB schema changes at startup           |
| ğŸ“¦ Data Export            | Export logs or backup data from volumes      |
| ğŸ§ª Batch Processing       | Run ML model training once per dataset       |
| ğŸ” Certificate Renewal    | Generate or renew SSL certs programmatically |
| ğŸ“¬ Send Emails or Reports | Trigger ad-hoc notification workflows        |

---

## ğŸ§¾ Basic Job Example

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: hello-job
spec:
  template:
    spec:
      containers:
      - name: hello
        image: busybox
        command: ["echo", "Hello from Job"]
      restartPolicy: Never
```

---

## ğŸ” Key Fields:

| Field           | Meaning                                                 |
| --------------- | ------------------------------------------------------- |
| `completions`   | Total successful runs needed (default: 1)               |
| `parallelism`   | How many pods can run in parallel                       |
| `backoffLimit`  | Max retries before job is marked as failed (default: 6) |
| `restartPolicy` | Should be `OnFailure` or `Never`                        |

---

## ğŸ§  Common Interview Questions

â“1. Whatâ€™s the difference between Job and CronJob?


> Job runs **once**.
> CronJob runs **on a schedule** (like `cron` in Linux).

â“2. What happens if a Job Pod crashes?
> Kubernetes retries it, up to `backoffLimit`.
> If still failing â†’ Job is marked as failed.

â“3. Can you run Jobs in parallel?
> âœ… Yes, use:

```yaml
parallelism: 3
completions: 6
```
This runs 3 at a time, until 6 finish successfully.

â“4. Will a Job clean up its pods?
> âŒ No. You must clean up Job pods or set `ttlSecondsAfterFinished`.

---

## âœ… Summary (Interview-style):

> A **Job** in Kubernetes runs one-off or batch tasks until they succeed.
> It supports retries, parallelism, and is ideal for tasks like DB migrations, backups, or report generation. For scheduled runs, use a **CronJob**.

==============================================================================

# PBD 

## ğŸ§· What is a **PDB**?
> A **Pod Disruption Budget (PDB)** protects critical workloads from **too many pods being evicted** during:
* Voluntary disruptions (like drain, upgrade, scaling)
* Node maintenance or cluster rebalancing


### âœ… Why use it?
- Without a PDB, **all pods of a deployment might be evicted at once**, causing downtime.
- PDB ensures that a **minimum number of pods stay available**.


## ğŸ§¾ Example:

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: myapp-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: myapp
```

ğŸ¯ This says: **"At least 2 pods must remain running during a disruption."**

---

## âš™ï¸ Key Options

| Field            | Meaning                                     |
| ---------------- | ------------------------------------------- |
| `minAvailable`   | Minimum number of **available pods**        |
| `maxUnavailable` | Maximum number of pods that **can go down** |
| `selector`       | Match the pods this PDB applies to          |

---

## ğŸ§  Interview Questions

â“1. Will PDB stop all disruptions?
> âŒ No â€” it only affects **voluntary** disruptions (e.g., `kubectl drain`, node upgrades).
> It doesn't block **crashes**, `kill -9`, or node failures.


â“2. Can PDB block node drain?
> âœ… Yes, if the drain would violate the PDB, it **blocks the eviction**.

â“3. Is PDB useful for single-pod workloads?
> âŒ No. PDB is only meaningful when you have **2+ replicas**.

## âœ… Final Summary (interview-style):
> A **PDB ensures high availability** by controlling how many pods can be disrupted at a time. Itâ€™s critical during maintenance to avoid **accidental full outage** of services.

====================================================================

# k8s STEPS FOR CI-CD
---------------------------

### âœ… 1. **Clone code (from Git)**

* Source code along with `Dockerfile` is pulled in Jenkins.

### âœ… 2. **Build artifact**

* Run:

  ```bash
  mvn clean install
  ```
* Generates a `.jar` in `target/`, e.g. `target/app.jar`

### âœ… 3. **Build Docker image**

* Your `Dockerfile` copies the jar:

  ```Dockerfile
  COPY target/app.jar /app/app.jar
  ```
* Build:

  ```bash
  docker build -t your-app:<tag> .
  ```

### âœ… 4. **Login, tag, push Docker image**

* Authenticate to registry (Docker Hub, ECR, etc.)

  ```bash
  docker login
  docker tag your-app:<tag> your-registry/your-app:<tag>
  docker push your-registry/your-app:<tag>
  ```

### âœ… 5. **Connect to Kubernetes**

* Set `KUBECONFIG` so Jenkins shell can talk to the cluster:

  ```bash
  export KUBECONFIG=/path/to/kubeconfig
  ```

### âœ… 6. **Update image in deployment**

* Use:

  ```bash
  kubectl set image deployment/<deployment-name> <container-name>=your-registry/your-app:<tag>
  ```

âœ… This performs a **rolling update** in Kubernetes.

==========================================================================================

# KUBE CONFIG

Sure! Here's a complete explanation of **`KUBECONFIG`** in simple, technical, and **interview-friendly** language ğŸ‘‡

---

## ğŸ“ What is `KUBECONFIG`?

> `KUBECONFIG` is an **environment variable** that tells the `kubectl` command **which cluster config file to use** to connect to Kubernetes.

By default, it uses:

```bash
~/.kube/config
```

But if you want to use a **different file or multiple configs**, you set `KUBECONFIG` manually.

---

## ğŸ§  Why it's important

It controls:

* âœ… Which **cluster** you talk to
* âœ… Which **user identity** is used (auth info)
* âœ… What **namespace** and **context** is active
* âœ… TLS settings, tokens, etc.

---

## ğŸ” How to use it

### ğŸ”¹ 1. Use a different config:

```bash
export KUBECONFIG=/path/to/custom/config.yaml
kubectl get pods
```

### ğŸ”¹ 2. Use **multiple config files**:

```bash
export KUBECONFIG=~/.kube/config:/path/to/staging.yaml:/path/to/dev.yaml
kubectl config view --merge
```

---

## ğŸ” What's inside a kubeconfig file?

Hereâ€™s a simplified version of `~/.kube/config`:

```yaml
apiVersion: v1
kind: Config

clusters:
- name: my-cluster
  cluster:
    server: https://api.k8s.example.com
    certificate-authority: ca.crt

users:
- name: admin
  user:
    token: <JWT-token>

contexts:
- name: admin@my-cluster
  context:
    cluster: my-cluster
    user: admin

current-context: admin@my-cluster
```

---

## âœ… Final Summary (Interview-Friendly):

> `KUBECONFIG` is an environment variable used by `kubectl` to locate one or more config files that store cluster connection details, credentials, and context.
> It's especially useful when you work with **multiple Kubernetes clusters** or CI/CD tools that use **custom kubeconfigs**.

---

Let me know if you want to generate a custom kubeconfig for a ServiceAccount or automate it using CI/CD!
